{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises due by EOD 2018.12.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this homework assignment we will work with deep learning libraries and `gpu` `ec2` instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## method of delivery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as mentioned in our first lecture, the method of delivery may change from assignment to assignment. we will include this section in every assignment to provide an overview of how we expect homework results to be submitted, and to provide background notes or explanations for \"new\" delivery concepts or methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this week you will be submitting the results of your homework via upload to your `s3` homework bucket\n",
    "\n",
    "summary:\n",
    "\n",
    "| exercise | deliverable | method of delivery |\n",
    "|----------|-------------|--------------------|\n",
    "| 1 | none | none |\n",
    "| 2 | an `environment.yml` file | uploaded to your `s3` homework bucket |\n",
    "| 3 | a `load_train_positions.py` file | uploaded to your `s3` homework bucket |\n",
    "| 4 | a `boston_keras.py` file | uploaded to your `s3` homework bucket |\n",
    "| 5 | a `results.csv` file | uploaded to your `s3` homework bucket |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 1: execute and read the updated deep learning lecture\n",
    "\n",
    "we're short on time now at the end of the year, and the deep learning lecture was only very recently finalized. to save us some class time and allow us to move on to `hadoop` before the end of the year, please read the remainder of the deep learning lecture, in which we cover `tensorflow` and `keras`.\n",
    "\n",
    "for this lecture in particular it will be important to **execute** the cells in the notebook instead of just reading the material.\n",
    "\n",
    "**choose one** of the following two options to execute the lectures\n",
    "\n",
    "\n",
    "## 1.1: run the notebook locally\n",
    "\n",
    "same ol' song and dance at this point -- download the file either directly from the `github` web interface or via `git pull`-ing the repository to your local desktop. then, in the directory containing the lecture (`014_deep_learning.ipynb`), run `jupyter notebook`.\n",
    "\n",
    "the environment which launches the `jupyter` server with that command must additionally have the following packages installed to properly execute all of the code in that `notebook`:\n",
    "\n",
    "+ `keras`\n",
    "+ `numpy`\n",
    "+ `plotly`\n",
    "+ `scikit-learn`\n",
    "+ `tensorflow`\n",
    "\n",
    "\n",
    "## 1.2: run the notebook using `google` `colab`\n",
    "\n",
    "either by installing the `chrome` browser extension [\"open in colab\"](https://chrome.google.com/webstore/detail/open-in-colab/iogfkhleblhcpcekbiedikdehleodpjo) or by navigiting to [google colab](https://colab.research.google.com) and opening the notebook `url` `https://github.com/rzl-ds/gu511/blob/master/014_deep_learning.ipynb` from `github`\n",
    "\n",
    "##### there is nothing to submit for this exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 2: install `tensorflow` on an `ec2` instance\n",
    "\n",
    "on your `ec2` instance, let's install the *non-`gpu`* `tensorflow` package (using `pip`) into a new `conda` environment\n",
    "\n",
    "+ create a new `conda` environment called `tf` with `python` version 3.6 (*not 3.7!*) and activate that environment\n",
    "+ install `tensorflow` using `pip` (not `conda install`)\n",
    "+ export that environment via `conda env export > environment.yml`\n",
    "\n",
    "##### upload your `environment.yml` file to your `s3` submission bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 3: load a `csv` as a `tensorflow` `dataset`\n",
    "\n",
    "let's use the `tensorflow` `dataset` `api` to load a large `csv` file as a tensor and do some simple calculations\n",
    "\n",
    "## 3.1: acquire the `csv`\n",
    "\n",
    "we will use the `1GB` `train_positions.csv` file I have made publically available on `s3`. download it to your `/tmp` directory on your `ubuntu` `ec2` server with the command\n",
    "\n",
    "```sh\n",
    "# the -P /tmp will save the resulting file in the /tmp directory\n",
    "wget https://s3.amazonaws.com/shared.rzl.gu511.com/train_positions/train_positions.csv -P /tmp\n",
    "```\n",
    "\n",
    "### 3.1.1: out of disk space?\n",
    "\n",
    "if in the process of downloading this file you run out of disk space, increase the size of your `ec2`'s hard disk (it's `ebs` volume) through the web console. on the `ec2` dashboard, click on the `ec2` instance with the hard drive you wish to expand, and in the bottom panel find the root device link. click on that link and a popup will show the `ebs` id link, click that link\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=1mW1APVujBcS_C31Vd_kWR1Q7Ox1g1pFo\" width=\"1000px\"></div>\n",
    "\n",
    "that link will have dropped you on the `ebs` id page. right click the volume row in the top panel and choose to modify the volume. modify the disk size by adding at least 1 GB.\n",
    "\n",
    "\n",
    "## 3.2: create a `CsvDataset` object\n",
    "\n",
    "in addition to the core `tensorflow` routines and `api`s, the `tensorflow` developers have a rigorous process for allowing developers to contribute new or experimental features. these new features are often saved in the `tf.contrib` namespace, but for datasets there is a special place for experimental (soon-to-be standard?) methods and classes: `tf.data.experimental`.\n",
    "\n",
    "one of the classes defined in that namespace is `tf.data.experimental.CsvDataset`. look at [the docstring](https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset)\n",
    "\n",
    "```python\n",
    "help(tf.data.experimental.CsvDataset)\n",
    "```\n",
    "\n",
    "\n",
    "### 3.2.1: initialization arguments\n",
    "\n",
    "a quick review of [the *initialization* function documentation](https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset#__init__) for this class (the one that is called to build our `CsvDataset` object)\n",
    "\n",
    "```python\n",
    "help(tf.data.experimental.CsvDataset.__init__)\n",
    "```\n",
    "\n",
    "shows us what arguments we have and gives us an idea of what we have to do to build this object.\n",
    "\n",
    "```python\n",
    "__init__(\n",
    "    filenames,\n",
    "    record_defaults,\n",
    "    compression_type=None,\n",
    "    buffer_size=None,\n",
    "    header=False,\n",
    "    field_delim=',',\n",
    "    use_quote_delim=True,\n",
    "    na_value='',\n",
    "    select_cols=None\n",
    ")\n",
    "```\n",
    "\n",
    "+ `filenames`: is a `tensor` of filenames as strings (it also accepts a single filename string, conveniently)\n",
    "+ `record_default`: a list of default values for incoming records\n",
    "    + each feature is represented by either a default value (e.g. '') if it *is not* required, or a `tensorflow` `dtype` if it *is* required\n",
    "+ `header`: a `bool` indicating whether or not the file has a `header` row\n",
    "\n",
    "we will need to specify values for those three arguments; the rest of the arguments can be left as defaults.\n",
    "\n",
    "\n",
    "#### 3.2.1.1: `record_defaults`\n",
    "\n",
    "check the first few records of the `csv` file with\n",
    "\n",
    "```sh\n",
    "head -n20 /tmp/train_positions.csv\n",
    "```\n",
    "\n",
    "the following table summarizes the columns, whether or not they contain null values, and the suggested default value. use this table to construct the `record_default` list\n",
    "\n",
    "| column name | contains `null` values | suggested default value |\n",
    "|-|-|-|\n",
    "| `carcount` | no | `tf.int32` |\n",
    "| `circuitid` | no | `tf.int32` |\n",
    "| `destinationstationcode` | no | `''` |\n",
    "| `directionnum` | no | `tf.int32` |\n",
    "| `linecode` | no | `''` |\n",
    "| `secondsatlocation` | no | `tf.int32` |\n",
    "| `servicetype` | no | `tf.string` |\n",
    "| `trainid` | no | `tf.string` |\n",
    "| `timestamp` | no | `tf.string` |\n",
    "\n",
    "\n",
    "### 3.2.2: invoking `CsvDataset`\n",
    "\n",
    "fill in the code below to create your dataset\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "filenames = #----------------#\n",
    "            # FILL THIS IN!! #\n",
    "            #----------------#\n",
    "record_defaults = #----------------#\n",
    "                  # FILL THIS IN!! #\n",
    "                  #----------------#\n",
    "header = #----------------#\n",
    "         # FILL THIS IN!! #\n",
    "         #----------------#\n",
    "        \n",
    "train_positions_dataset = tf.data.experimental.CsvDataset(\n",
    "    filenames=filenames,\n",
    "    record_defaults=record_defaults,\n",
    "    header=header,\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "## 3.3: create a `batch`ed `iterator`\n",
    "\n",
    "using your `train_positions_dataset` object's [`.batch`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset#batch) and [`.make_one_shot_iterator`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset#make_one_shot_iterator) methods, create an iterator that has a batch size of 3 by filling in the code below\n",
    "\n",
    "```python\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "# make a batched dataset\n",
    "tp_batched = #----------------#\n",
    "             # FILL THIS IN!! #\n",
    "             #----------------#\n",
    "\n",
    "from tensorflow.python.data.ops.dataset_ops import BatchDataset\n",
    "assert isinstance(tp_batched, BatchDataset)\n",
    "\n",
    "# make a one-shot iterator from your batched dataset\n",
    "tp_batched_oneshot = #----------------#\n",
    "                     # FILL THIS IN!! #\n",
    "                     #----------------#\n",
    "\n",
    "from tensorflow.python.data.ops.iterator_ops import Iterator\n",
    "assert isinstance(tp_batched_oneshot, Iterator)\n",
    "```\n",
    "\n",
    "you can verify that this worked by executing\n",
    "\n",
    "\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "    iterator = tp_batched_oneshot\n",
    "    next_element = iterator.get_next()\n",
    "    elem = sess.run(next_element)\n",
    "assert elem[1].tolist() == [2009, 1912, 1480]\n",
    "assert elem[-2].tolist() == [b'067', b'175', b'182']\n",
    "```\n",
    "\n",
    "\n",
    "## 3.4: put it together\n",
    "\n",
    "fill in the following block of `python` code and save the results as `load_train_positions.py`\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.data.ops.dataset_ops import BatchDataset\n",
    "from tensorflow.python.data.ops.iterator_ops import Iterator\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "def build_train_positions_iterator():\n",
    "    filenames = #----------------#\n",
    "                # FILL THIS IN!! #\n",
    "                #----------------#\n",
    "    record_defaults = #----------------#\n",
    "                      # FILL THIS IN!! #\n",
    "                      #----------------#\n",
    "    header = #----------------#\n",
    "             # FILL THIS IN!! #\n",
    "             #----------------#\n",
    "\n",
    "    train_positions_dataset = tf.data.experimental.CsvDataset(\n",
    "        filenames=filenames,\n",
    "        record_defaults=record_defaults,\n",
    "        header=header,\n",
    "    )\n",
    "\n",
    "    # make a batched dataset\n",
    "    tp_batched = #----------------#\n",
    "                 # FILL THIS IN!! #\n",
    "                 #----------------#\n",
    "\n",
    "    assert isinstance(tp_batched, BatchDataset)\n",
    "\n",
    "    # make a one-shot iterator from your batched dataset\n",
    "    tp_batched_oneshot = #----------------#\n",
    "                         # FILL THIS IN!! #\n",
    "                         #----------------#\n",
    "\n",
    "    assert isinstance(tp_batched_oneshot, Iterator)\n",
    "\n",
    "    return tp_batched_oneshot\n",
    "\n",
    "\n",
    "def validate():\n",
    "    tp_batched_oneshot = build_train_positions_iterator()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        iterator = tp_batched_oneshot\n",
    "        next_element = iterator.get_next()\n",
    "        elem = sess.run(next_element)\n",
    "        \n",
    "    assert elem[1].tolist() == [2009, 1912, 1480]\n",
    "    assert elem[-2].tolist() == [b'067', b'175', b'182']\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    validate()\n",
    "```\n",
    "\n",
    "if everything works as expected, you should be able to run (from the `bash` command line)\n",
    "\n",
    "```sh\n",
    "python load_train_positions.py\n",
    "```\n",
    "\n",
    "and the result will be nothing -- no `python` errors\n",
    "\n",
    "\n",
    "##### upload your `load_train_positions.py` file to your `s3` homework submission bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 4: simple `keras` models\n",
    "\n",
    "let's create a pair of simple `keras` models to predict housing prices.\n",
    "\n",
    "\n",
    "## 4.1: load the Boston housing price dataset\n",
    "\n",
    "`keras` provides built-in access to a number of datasets via the [`keras.datasets`](https://keras.io/datasets/#boston-housing-price-regression-dataset) module. we will use that to load train and test data in a format that is immediately consumable in a `keras` model\n",
    "\n",
    "```python\n",
    "from tensorflow import keras\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.boston_housing.load_data()\n",
    "```\n",
    "\n",
    "additionaly, let's normalize the predictor data:\n",
    "\n",
    "```python\n",
    "mean = x_train.mean(axis=0)\n",
    "std = x_train.std(axis=0)\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = (x_test - mean) / std\n",
    "```\n",
    "\n",
    "\n",
    "## 4.2: a linear model\n",
    "\n",
    "\n",
    "### 4.2.1: build the model\n",
    "\n",
    "we can build a linear regression in `keras` quite easily -- a linear regression is simply a\n",
    "\n",
    "+ one-layer `Sequential` model\n",
    "+ in which the one layer\n",
    "    + is `Dense`\n",
    "    + takes our `x` datasets as `inputs` (this defines `input_dim`)\n",
    "    + has only one set of weights (i.e. is only one node tall)\n",
    "    + has a `linear` activation (this is the default activation value, so no argument is necessary)\n",
    "\n",
    "fill in the below snippet to create a linear model\n",
    "\n",
    "```python\n",
    "linear_model = #----------------#\n",
    "               # FILL THIS IN!! #\n",
    "               #----------------#\n",
    "```\n",
    "\n",
    "after doing so, you should be able to run\n",
    "\n",
    "```python\n",
    "linear_model.summary()\n",
    "```\n",
    "\n",
    "and see (`dense_17` may have a different number for you)\n",
    "\n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_17 (Dense)             (None, 1)                 14        \n",
    "=================================================================\n",
    "Total params: 14\n",
    "Trainable params: 14\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```\n",
    "\n",
    "\n",
    "### 4.2.2: `compile`\n",
    "\n",
    "furthermore, we want to `compile` this model to use the `adam` optimizer algorithm to optimize a `mse` `loss` funciton. let's track the `mean_absolute_error` `metric` as well\n",
    "\n",
    "```python\n",
    "linear_model.compile(\n",
    "    loss=,  # FILL THIS IN!!\n",
    "    optimizer=,  # FILL THIS IN!!\n",
    "    metrics=,  # FILL THIS IN!!\n",
    ")\n",
    "```\n",
    "\n",
    "### 4.2.3: `fit`\n",
    "\n",
    "finally, let's fit our training dataset. let's use validation within each `epoch` with a `validation_split` of 0.05. also, in order to treat both model types on an equal footing, rather than stop after a fixed number of epochs we wil stop after our best `mse` value. to do this, we will use `EarlyStopping` and `ModelCheckpoint` `callback`s.\n",
    "\n",
    "```python\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.01,\n",
    "    patience=100\n",
    ")\n",
    "\n",
    "mc_callback = keras.callbacks.ModelCheckpoint(\n",
    "    'linear.hdf5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "callbacks = [es_callback, mc_callback]\n",
    "```\n",
    "\n",
    "set the `validation_split` value to 0.05, set the `verbose` value to 0, the number of `epoch`s to be 10,000, and add the `callbacks` to fit on the `x` and `y` train datasets\n",
    "\n",
    "```python\n",
    "linear_model.fit(\n",
    "    # FILL THIS IN!!\n",
    ")\n",
    "```\n",
    "\n",
    "### 4.2.4: `evaluate`\n",
    "\n",
    "load the saved best dataset and view the ultimate accuracy of this model on the held-out test data:\n",
    "\n",
    "```python\n",
    "best_linear_model = keras.models.load_model('linear.hdf5')\n",
    "linear_test_mse, linear_test_mae = best_linear_model.evaluate(x_test, y_test)\n",
    "print(\"linear test mse: {}\".format(linear_test_mse))\n",
    "print(\"linear test mae: {}\".format(linear_test_mae))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 4.3: a deep neural net model\n",
    "\n",
    "let's repeat the above but with a neural network architecture. create a new `Sequential` model with the following:\n",
    "\n",
    "+ several layers\n",
    "    + one 20-node layer with `relu` activation and `input_dim` determined by the shape of `x_test`\n",
    "    + one 10-node layer with `relu` activation\n",
    "    + one 6-node layer with `relu` activation\n",
    "    + one 1-node output layer with the default activation\n",
    "+ compile with\n",
    "    + an `adam` optimizer\n",
    "    + a `mse` loss\n",
    "    + a `mean_absolute_error` metric\n",
    "+ fit with\n",
    "    + 10,000 `epochs`\n",
    "    + a `validation_split` of 0.05\n",
    "\n",
    "```python\n",
    "dnn_model = #----------------#\n",
    "            # FILL THIS IN!! #\n",
    "            #----------------#\n",
    "\n",
    "dnn_model.compile(\n",
    "    loss=,  # FILL THIS IN!!\n",
    "    optimizer=,  # FILL THIS IN!!\n",
    "    metrics=,  # FILL THIS IN!!\n",
    ")\n",
    "\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.01,\n",
    "    patience=100\n",
    ")\n",
    "\n",
    "mc_callback = keras.callbacks.ModelCheckpoint(\n",
    "    'dnn.hdf5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "callbacks = [es_callback, mc_callback]\n",
    "\n",
    "dnn_model.fit(\n",
    "    # FILL THIS IN!!\n",
    ")\n",
    "\n",
    "best_dnn_model = keras.models.load_model('dnn.hdf5')\n",
    "dnn_test_mse, dnn_test_mae = best_dnn_model.evaluate(x_test, y_test)\n",
    "print(\"dnn test mse: {}\".format(dnn_test_mse))\n",
    "print(\"dnn test mae: {}\".format(dnn_test_mae))\n",
    "```\n",
    "\n",
    "\n",
    "## 4.4: bring it all together\n",
    "\n",
    "fill in all of the above in one file named `boston_keras.py`\n",
    "\n",
    "```python\n",
    "from tensorflow import keras\n",
    "\n",
    "def main():\n",
    "    # load boston data\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.boston_housing.load_data()\n",
    "    \n",
    "    # standardize\n",
    "    mean = x_train.mean(axis=0)\n",
    "    std = x_train.std(axis=0)\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "    \n",
    "    # linear model ------------------------------------------------------------\n",
    "    print('{:-<80}'.format('linear model '))\n",
    "    \n",
    "    # init\n",
    "    linear_model = #----------------#\n",
    "                   # FILL THIS IN!! #\n",
    "                   #----------------#\n",
    "\n",
    "    print(linear_model.summary())\n",
    "    \n",
    "    # compile\n",
    "    linear_model.compile(\n",
    "        loss=,  # FILL THIS IN!!\n",
    "        optimizer=,  # FILL THIS IN!!\n",
    "        metrics=,  # FILL THIS IN!!\n",
    "    )\n",
    "    \n",
    "    # linear callbacks\n",
    "    es_callback = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0.01,\n",
    "        patience=1000\n",
    "    )\n",
    "\n",
    "    mc_callback = keras.callbacks.ModelCheckpoint(\n",
    "        'linear.hdf5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "\n",
    "    callbacks = [es_callback, mc_callback]\n",
    "\n",
    "    # fit\n",
    "    linear_model.fit(\n",
    "        # FILL THIS IN!!\n",
    "    )\n",
    "    \n",
    "    # evaluate\n",
    "    best_linear_model = keras.models.load_model('linear.hdf5')\n",
    "    linear_test_mse, linear_test_mae = best_linear_model.evaluate(x_test, y_test)\n",
    "    print(\"linear test mse: {}\".format(linear_test_mse))\n",
    "    print(\"linear test mae: {}\".format(linear_test_mae))\n",
    "    \n",
    "    # dnn model ---------------------------------------------------------------\n",
    "    print('{:-<80}'.format('dnn model '))\n",
    "\n",
    "    # init\n",
    "    dnn_model = #----------------#\n",
    "                # FILL THIS IN!! #\n",
    "                #----------------#\n",
    "\n",
    "    print(dnn_model.summary())\n",
    "            \n",
    "    # compile\n",
    "    dnn_model.compile(\n",
    "        loss=,  # FILL THIS IN!!\n",
    "        optimizer=,  # FILL THIS IN!!\n",
    "        metrics=,  # FILL THIS IN!!\n",
    "    )\n",
    "    \n",
    "    # dnn callbacks\n",
    "    es_callback = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0.01,\n",
    "        patience=1000\n",
    "    )\n",
    "\n",
    "    mc_callback = keras.callbacks.ModelCheckpoint(\n",
    "        'dnn.hdf5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "\n",
    "    callbacks = [es_callback, mc_callback]\n",
    "\n",
    "    # fit\n",
    "    dnn_model.fit(\n",
    "        # FILL THIS IN!!\n",
    "    )\n",
    "\n",
    "    # evaluate\n",
    "    best_dnn_model = keras.models.load_model('dnn.hdf5')\n",
    "    dnn_test_mse, dnn_test_mae = best_dnn_model.evaluate(x_test, y_test)\n",
    "    print(\"dnn test mse: {}\".format(dnn_test_mse))\n",
    "    print(\"dnn test mae: {}\".format(dnn_test_mae))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n",
    "\n",
    "\n",
    "##### upload your completed `boston_keras.py` file to your `s3` homework submission bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 5: benchmark differences in performance using `gpu`s\n",
    "\n",
    "let's spin up a `gpu` `ec2` instance and do a simple benchmark to see the performance improvements available via `gpu`s\n",
    "\n",
    "\n",
    "## 5.1: `gpu`s are expensive!\n",
    "\n",
    "go check [the per-hour price](https://aws.amazon.com/ec2/pricing/on-demand/) of `gpu` compute for a `p3.2xlarge` instance in the US East (Virginia) region. as of writing, it is 3.06 USD per hour.\n",
    "\n",
    "we don't want to leave that on for long, so let's make this quick!\n",
    "\n",
    "\n",
    "## 5.2: spin up a `p3.2xlarge` instance\n",
    "\n",
    "`aws` has already created a deep learning `ami` for us, so let's use it and save time (and money) on downloads.\n",
    "\n",
    "+ open the `ec2` web console and create a new instance\n",
    "+ on page 1 (select `ami`)\n",
    "    + select the \"AWS Marketplace\" menu on the left side\n",
    "    + search for \"Deep Learning AMI (Ubuntu)\"\n",
    "    + select the first one, titled \"Deep Learning AMI (Ubuntu)\" (*doesn't contain \"base\" in the title*)\n",
    "    + press \"continue\"\n",
    "+ on page 2 (choose an instance type)\n",
    "    + find `p3.2xlarge` and select it\n",
    "    + click \"review and launch\"\n",
    "+ launch the instance\n",
    "    + make sure you have your `ssh` key saved somewhere easy!\n",
    "\n",
    "\n",
    "## 5.3: log in\n",
    "\n",
    "after your `ec2` instance is up and running, log in to it using username `ubuntu` and providing the path to the private key `.pem` file you either downloaded just now or when you created that key pair for a previous `ec2` instance\n",
    "\n",
    "if you don't know where this key file is, *terminate the instance* (right click > instance state > terminate) and start over.\n",
    "\n",
    "\n",
    "## 5.4: download a benchmark\n",
    "\n",
    "download [this public `gist`](https://gist.github.com/RZachLamberty/fe8e05060b809e90fd2722feeb80fcda) to your new `ec2` instance:\n",
    "\n",
    "```sh\n",
    "wget https://gist.githubusercontent.com/RZachLamberty/fe8e05060b809e90fd2722feeb80fcda/raw/0e0834395ba60c317168bdfdeb0c898afac6013b/cpu_gpu_benchmark.py\n",
    "```\n",
    "\n",
    "\n",
    "## 5.5: activate an environment\n",
    "\n",
    "the good folks at `aws` have pre-configured this `ami` with a ton of different deep-learning-capable environments, and the commands for entering any one of them are printed out when you log in to the `ami`. one in particular is for us right now:\n",
    "\n",
    "```\n",
    "for TensorFlow(+Keras2) with Python3 (CUDA 9.0 and Intel MKL-DNN) ___________________________ source activate tensorflow_p36\n",
    "```\n",
    "\n",
    "run that command to activate that environment\n",
    "\n",
    "\n",
    "## 5.6: run the benchmark\n",
    "\n",
    "now that we have everything we need already installed (thanks, `aws` `ami`!), go ahead and run the benchmark script:\n",
    "\n",
    "```sh\n",
    "python cpu_gpu_benchmark.py\n",
    "```\n",
    "\n",
    "> **note**: the first time you run on this machine, the process of initializing `tensorflow` for the first time may require enough overhead to cause an error in the benchmarking script. you will see a `ValueError: Empty data passed with indices specified.` error. if you see this, just run the benchmark script again. if you see it more than three times, terminate your instance and send me an email.\n",
    "\n",
    "the final output will be a dataframe which lists the amount of time it took to create random matrices of increasing sizes and multiply them, as well as the ratio of the speeds for the different devices for each operation.\n",
    "\n",
    "it will also output a `csv` named `results.csv`. download that file (use `scp` to copy it from your `ec2` to your laptop, or just open it, highlight, and copy-paste to a local file)\n",
    "\n",
    "\n",
    "## 5.7: TERMINATE YOUR `gpu` INSTANCE!!\n",
    "\n",
    "don't forget to go back into the `ec2` web console and terminate your instance (right click > instance state > terminate).\n",
    "\n",
    "\n",
    "##### upload `results.csv` to your `s3` homework submission bucket"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
