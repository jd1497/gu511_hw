{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises due by EOD <span style=\"color:red\">2018.12.13<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this homework assignment we will work with `hadoop` and `spark` as well as `github` `pull request`s.\n",
    "\n",
    "for several reasons:\n",
    "\n",
    "+ because the final will be assigned on the 13th and we must push answers to you and grade them asap\n",
    "+ `emr` is crazy expensive\n",
    "\n",
    "this homework assignment will be a bit smaller and easier than others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## method of delivery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as mentioned in our first lecture, the method of delivery may change from assignment to assignment. we will include this section in every assignment to provide an overview of how we expect homework results to be submitted, and to provide background notes or explanations for \"new\" delivery concepts or methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this week you will be submitting the results of your homework via upload to your `s3` homework bucket and `commit`s on `github`\n",
    "\n",
    "summary:\n",
    "\n",
    "| exercise | deliverable | method of delivery |\n",
    "|----------|-------------|--------------------|\n",
    "| 1 | none | none |\n",
    "| 2 | none | none |\n",
    "| 3 | a file `super_hadooper.csv ` | uploaded to your `s3` homework submission bucket |\n",
    "| 4 | a file `download_usaspending_sample.sh` | uploaded to your `s3` homework submission bucket |\n",
    "| 5 | a file `load_usaspending_to_hdfs.sh` | uploaded to your `s3` homework submission bucket |\n",
    "| 6 | none | none |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 1: set up a virtual development environment\n",
    "\n",
    "the appendix of the `015_hadoop.ipynb` lecture includes several ways of creating a `hadoop` development environment. as much as possible, we will try to execute our `hadoop` commands in a local development environment (using `docker` or virtual machines) until we know they work; then we can pay the big bucks for an `emr` environment when needed.\n",
    "\n",
    "if you run into issues, please check the lecture appendix first, and then reach out to us.\n",
    "\n",
    "\n",
    "## 1.1: updating our `docker` vm memory size\n",
    "\n",
    "these images require a log of memory, and the default amount of memory set aside for `docker` `containers` is 2GB\n",
    "\n",
    "if you are using a windows or mac computer, increase your vm memory as discussed [here](https://stackoverflow.com/questions/44533319/how-to-assign-more-memory-to-docker-container). I recommend `8 GB` if your computer has more than `8GB` of memory, otherwise `4 GB`.\n",
    "\n",
    "for linux, just increase memory by invoking `docker run` (when you do) with the command line flag `--memory=8g`\n",
    "\n",
    "\n",
    "## 1.2: finding good local ports\n",
    "\n",
    "let's create a `docker` `container` running the `cloudera` `hadoop` distribution (version 5.7 quickstart method). the main thing to figure out is what local ports we can map to the `container`'s internal ports. go to each of the following and see if any web app is running there:\n",
    "\n",
    "+ port `8888`: http://localhost:8888\n",
    "+ port `7180`: http://localhost:7180\n",
    "+ port `8880`: http://localhost:8880\n",
    "\n",
    "hopefully there are no applications running on those ports, and what we see at every url is\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=13Neuqun5a7GZQRpnWmZ5U6Yeh-5V3hGr\"></div>\n",
    "\n",
    "if you see an application (especially `jupyter` running on port `8888`, its default port) simply take the \"problem\" port and add 1 and try again (e.g. is anything running on http://localhost:8889? how about http://localhost:8890?). keep going until you have a series of unoccupied ports\n",
    "\n",
    "\n",
    "## 1.3: running the `docker` `image`\n",
    "\n",
    "take those unoccupied ports and fill them in below, then run the commands from the command line\n",
    "\n",
    "```sh\n",
    "HOST_PORT_HUE=9999\n",
    "HOST_PORT_CLOUDERA_MANAGER=7180\n",
    "HOST_PORT_TUTORIAL=8880\n",
    "\n",
    "# if copy-paste doesn't work, try the one-line version below\n",
    "docker run \\\n",
    "    --hostname=quickstart.cloudera \\\n",
    "    --privileged=true \\\n",
    "    --rm \\\n",
    "    -it \\\n",
    "    -p $HOST_PORT_HUE:8888 \\\n",
    "    -p $HOST_PORT_CLOUDERA_MANAGER:7180 \\\n",
    "    -p $HOST_PORT_TUTORIAL:80 \\\n",
    "    cloudera/quickstart:latest \\\n",
    "    /usr/bin/docker-quickstart\n",
    "```\n",
    "\n",
    "if the copy-paste didn't work, here's one that's all on one line (but less readable!)\n",
    "\n",
    "```sh\n",
    "# one-line version\n",
    "docker run --hostname=quickstart.cloudera --privileged=true --rm -it -p $HOST_PORT_HUE:8888 -p $HOST_PORT_CLOUDERA_MANAGER:7180 -p $HOST_PORT_TUTORIAL:80 cloudera/quickstart:latest /usr/bin/docker-quickstart\n",
    "```\n",
    "\n",
    "this should download the `cloudera` `quickstart` `docker` `image` and then kick off a long stream of startup scripts. you will know you are done when the terminal looks like this:\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=1TzvLGbGkvJlID19stE8NhI17aaLTP3a9\"></div>\n",
    "\n",
    "you can execute `hadoop` commands inside this `container` now:\n",
    "\n",
    "```sh\n",
    "hadoop fs -ls /\n",
    "```\n",
    "\n",
    "you can also verify that it all worked by going to `http://localhost:8888`, where `8888` is replaced with whatever port you found up above. if there is a web applciation `hue` running there:\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=1zbnV1GmCBHx6XLGur17HLS3MVg4w6ReI\"></div>\n",
    "\n",
    "the username and password for this application are both `cloudera`\n",
    "\n",
    "you're good to go!\n",
    "\n",
    "when you're done with the `container` (after this assignment) exit the terminal with `exit`\n",
    "\n",
    "##### there is nothing to submit here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 1: set up a virtual development environment <span style=\"color:red;font-weight:bold\">ANSWERS</span>\n",
    "\n",
    "follow the instructions above and in the lecture materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid lightgrey;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 2: read the rest of the `hadoop` lecture\n",
    "\n",
    "read the rest of the `015_hadoop.ipynb` lecture. the assignments below will leverage some of the material presented there -- especially `hadoop fs` commands and `pyspark`.\n",
    "\n",
    "the commands in the lecture are specifically designed to work in the `emr` environment -- which is slightly different than the `cloudera` environment. if you run into issues running commands, please reach out to me directly\n",
    "\n",
    "##### there is nothing to submit for this question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 2: read the rest of the `hadoop` lecture <span style=\"color:red;font-weight:bold\">ANSWERS</span>\n",
    "\n",
    "no answers, just read the lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid lightgrey;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 3: re-learning `bash` for `hdfs`\n",
    "\n",
    "## 3.1: setup\n",
    "\n",
    "do the following in your *cloudera development environment*!\n",
    "\n",
    "let's create some giberish files on our local file system for use in interacting with the `hdfs`. simply execute the following code to create a number of local files we can reference in our commands.\n",
    "\n",
    "*note: you may need to install `wget` with `yum install wget`*\n",
    "\n",
    "```bash\n",
    "mkdir /tmp/cachenet\n",
    "cd /tmp/cachenet\n",
    "wget -x www.google.com\n",
    "wget -x www.nytimes.com\n",
    "wget -x www.twitter.com/i/moments\n",
    "wget -x www.twitter.com/i/notifications\n",
    "wget -x www.facebook.com\n",
    "wget -x www.youtube.com\n",
    "wget -x www.youtube.com/feed/trending\n",
    "wget -x www.espn.com\n",
    "wget -x en.wikipedia.org\n",
    "wget -x en.wikipedia.org/wiki/L33T\n",
    "wget -x www.reddit.com\n",
    "wget -x www.reddit.com/r/all\n",
    "wget -x www.reddit.com/r/datascience\n",
    "wget -x www.reddit.com/r/python\n",
    "cd ~\n",
    "```\n",
    "\n",
    "\n",
    "## 3.2: commands\n",
    "\n",
    "in the left column of the table below I have described some pretty common filesystem operations, and listed in the second column the `bash` commands to execute them. fill in the comparable commands (i.e. those you enter at a bash prompts) to perform the same general tasks on the `hdfs`. save the resulting three-column table as a `csv` with name `super_hadooper.csv`\n",
    "\n",
    "\n",
    "### 3.2.1: quick note: running as an `hdfs` super user\n",
    "\n",
    "some commands will require you to be running as the `hdfs` user instead of the `root` user -- this is a file permission control built in to `hadoop`. some distributions have the main `hadoop` user named `hadoop`; others named `hdfs`. `cloudera` has chosen `hdfs`.\n",
    "\n",
    "to execute commands as the `hdfs` user, it is necessary to prefix your commands with `sudo -u hdfs`, which will execute the command as the `hdfs` linux user account (which is the `hdfs` super user). given a hadoop command you know, you can execute that same command by prefacing it like:\n",
    "\n",
    "```\n",
    "sudo -u hdfs hadoop [HADOOP CMD HERE]\n",
    "```\n",
    "\n",
    "e.g. `hadoop fs -ls /` would turn into `sudo -u hdfs hadoop fs -ls /`\n",
    "\n",
    "\n",
    "### 3.2.2: read the manual!\n",
    "\n",
    "feel free to check out the `hadoop fs` documentation. make sure you get the right version! check your `hadoop` version with `hadoop -version`. some of the versions we have access to*:\n",
    "\n",
    "+ [the `cloudera` `docker` container: 2.6.0](http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/FileSystemShell.html)\n",
    "+ [the `aws` `emr` instance: 2.8.5](http://hadoop.apache.org/docs/r2.8.5/hadoop-project-dist/hadoop-common/FileSystemShell.html)\n",
    "\n",
    "\n",
    "### 3.2.3: the command table\n",
    "\n",
    "| task description | bash command | `hadoop` command |\n",
    "|-|-|-|\n",
    "| ask for help re: the `ls` command | ls --help | |\n",
    "| make a directory named `/tmp` | `mkdir -p /tmp` | |\n",
    "| create an empty file `/tmp/test` | `touch /tmp/test` | |\n",
    "| list the contents of `/tmp` | `ls /tmp` | |\n",
    "| add group write permission for `/tmp/test` | `chmod g+w /tmp/test` | |\n",
    "| change the owner of `/tmp/test` to `hadoop` | `chown hadoop /tmp/test` | |\n",
    "| remove a file `/tmp/test` which you don't own | `sudo rm /tmp/test` | |\n",
    "| copy local `/tmp/cachenet` to `hdfs` | no meaningful analog | |\n",
    "| print the final lines of `/tmp/cachenet/www.nytimes.com/index.html` | `tail /tmp/cachenet/www.nytimes.com/index.html` | |\n",
    "| echo `hello world` to `/tmp/hello_world.txt` | `echo hello world > /tmp/hello_world.txt` | |\n",
    "| print `/tmp/hello_world.txt` to `stdout` | `cat /tmp/hello_world.txt` | |\n",
    "| get the free and used space | `df -h` | |\n",
    "| count the files and dirs under `/tmp` | no equivalent! |\n",
    "| remove `/tmp/hello_world.txt` | `rm /tmp/hello_world.txt` | |\n",
    "| recursively remove directory `/tmp/cachenet` | `rm -r /tmp/cachenet` | |\n",
    "\n",
    "\n",
    "##### upload `super_hadooper.csv` to your `s3` bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 3: re-learning `bash` for `hdfs` <span style=\"color:red;font-weight:bold\">ANSWERS</span>\n",
    "\n",
    "| task description | bash command | `hadoop` command |\n",
    "|-|-|-|\n",
    "| ask for help re: the `ls` command | ls --help | `hadoop fs -help ls` |\n",
    "| make a directory named `/tmp` | `mkdir -p /tmp` | `hadoop fs -mkdir -p /tmp` |\n",
    "| create an empty file `/tmp/test` | `touch /tmp/test` | `hadoop fs -touchz /tmp/test` |\n",
    "| list the contents of `/tmp` | `ls /tmp` | `hadoop fs -ls /tmp` |\n",
    "| add group write permission for `/tmp/test` | `chmod g+w /tmp/test` | `hadoop fs -chmod g+w /tmp/test` |\n",
    "| change the owner of `/tmp/test` to `hadoop` | `chown hadoop /tmp/test` | `hadoop fs -chown hadoop /tmp/test` |\n",
    "| remove a file `/tmp/test` which you don't own | `sudo rm /tmp/test` | `hadoop fs -rm /tmp/test` |\n",
    "| copy local `/tmp/cachenet` to `hdfs` | no meaningful analog | `hadoop fs -put /tmp/cachenet /tmp` |\n",
    "| print the final lines of `/tmp/cachenet/www.nytimes.com/index.html` | `tail /tmp/cachenet/www.nytimes.com/index.html` | `hadoop fs -tail /tmp/cachenet/www.nytimes.com/index.html` |\n",
    "| echo `hello world` to `/tmp/hello_world.txt` | `echo hello world > /tmp/hello_world.txt` | `echo hello world `&#124;` hadoop fs -put - /tmp/hello_world.txt` |\n",
    "| print `/tmp/hello_world.txt` to `stdout` | `cat /tmp/hello_world.txt` | `hadoop fs -cat /tmp/hello_world.txt` |\n",
    "| get the free and used space | `df -h` | `hadoop fs -df -h` |\n",
    "| count the files and dirs under `/tmp` | no easy analog | `hadoop fs -count /tmp/` |\n",
    "| remove `/tmp/hello_world.txt` | `rm /tmp/hello_world.txt` | `hadoop fs -rm /tmp/hello_world.txt` |\n",
    "| recursively remove directory `/tmp/cachenet` | `rm -r /tmp/cachenet` | `hadoop fs -rm -r /tmp/cachenet` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid lightgrey;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 4: downloading a sample `usaspending` dataset\n",
    "\n",
    "## 4.1: background\n",
    "\n",
    "[USA Spending](https://www.usaspending.gov/#/about) is an excellent open data source which offers details on government expenditures (e.g. IT accessory purchases, contract work). the datasets are freely available for download, either as one-off files (see https://www.usaspending.gov/#/download_center/award_data_archive), or as a `postgres` database (see [this page](https://files.usaspending.gov/database_download/)).\n",
    "\n",
    "let's get a file to work with. we want one that is small enough we can still work with it in development environments like our `docker` containers or our `vm`s. the entire dataset is around 90GB, so something we would probably want to work with in a database or distributed environment like `hadoop`\n",
    "\n",
    "\n",
    "## 4.2: downloading\n",
    "\n",
    "on [the award data archive page](https://www.usaspending.gov/#/download_center/award_data_archive), make sure the \"Fiscal Year\" dropdown is set to 2019. the top line in the table of links will be something like `2019_all_Contracts_Full_YYYYMMDD.zip`, where (as of writing) the `YYYYMMDD` \"as of\" date is `20181114` -- that date will update as the underlying datasets are modified and updated.\n",
    "\n",
    "as of writing, the 2019 contracts zip file `url` was: https://files.usaspending.gov/award_data_archive/2019_all_Contracts_Full_20181114.zip\n",
    "\n",
    "1. write a simple `curl` or `wget` command to download this to a linux instance\n",
    "1. write a simple `unzip` statement to unzip that file into a `csv`\n",
    "\n",
    "\n",
    "## 4.3: record format\n",
    "\n",
    "there is a complete data dictionary for this record online [here](https://www.usaspending.gov/#/download_center/data_dictionary), but the most important fields for our interest are:\n",
    "\n",
    "+ unique record identifiers\n",
    "    + `parent_award_id`: \"The identifier of the procurement award under which the specific award is issued (such as a Federal Supply Schedule). Term currently applies to procurement actions only\"\n",
    "    + `award_id_piid`: \"The unique identifier of the specific award being reported.\"\n",
    "    + `modification_number`: \"The identifier of an action being reported that indicates the specific subsequent change to the initial award.\"\n",
    "    + `transaction_number`: \"Tie Breaker for legal, unique transactions that would otherwise have the same key.\"\n",
    "+ other useful fields\n",
    "    + `action_date`: \"The date the action being reported was issued / signed by the Government or a binding agreement was reached.\"\n",
    "    + `last_modified_date`: \"The last modified date captures the change date.\"\n",
    "    + `current_total_value_of_award`: \"Total amount obligated to date on a contract, including the base and exercised options.\"\n",
    "    + `awarding_agency`: \"The name associated with a department or establishment of the Government as used in the Treasury Account Fund Symbol (TAFS).\"\n",
    "    + `recipient_name`: \"The name of the awardee or recipient that relates to the unique identifier. For U.S. based companies, this name is what the business ordinarily files in formation documents with individual states (when required).\"\n",
    "\n",
    "\n",
    "## 4.4: a simple in-memory filter\n",
    "\n",
    "this dataset includes multiple snapshots throught time of expenditures -- as contracts get updated, we get new *modifications* and we get new `modification_number` values. for example, given the downloaded `csv` as of part 2 above:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('2019_all_Contracts_Full_20181115_1.csv')\n",
    "\n",
    "# a special award with multiple modifications\n",
    "multi_mods = df[\n",
    "    (df.parent_award_id == 'GS07F0284N')\n",
    "    & (df.award_id_piid == 'DJJ15FUSA830157')\n",
    "]\n",
    "multi_mods[[\n",
    "    'parent_award_id', 'award_id_piid', 'modification_number',\n",
    "    'transaction_number', 'current_total_value_of_award', 'action_date',\n",
    "    'last_modified_date', 'awarding_agency_name', 'recipient_name', \n",
    "]].sort_values(by='last_modified_date')\n",
    "```\n",
    "\n",
    "will result in the following table:\n",
    "\n",
    "| index | parent_award_id | award_id_piid | modification_number | transaction_number | current_total_value_of_award | action_date | last_modified_date | awarding_agency_name | recipient_name |\n",
    "|-|-|-|-|-|-|-|-|-|-|\n",
    "9914 | GS07F0284N | DJJ15FUSA830157 | P00001 | 0.0 | 174724.59 | 2018-11-05 | 2018-11-05 15:59:15 | DEPARTMENT OF JUSTICE (DOJ) | ALL U NEED TEMPORARY SERVICES |\n",
    "10811 | GS07F0284N | DJJ15FUSA830157 | P00002 | 0.0 | 174724.59 | 2018-11-05 | 2018-11-05 16:06:42 | DEPARTMENT OF JUSTICE (DOJ) | ALL U NEED TEMPORARY SERVICES |\n",
    "9681 | GS07F0284N | DJJ15FUSA830157 | P00003 | 0.0 | 174724.59 | 2018-11-05 | 2018-11-05 16:13:37 | DEPARTMENT OF JUSTICE (DOJ) | ALL U NEED TEMPORARY SERVICES |\n",
    "9843 | GS07F0284N | DJJ15FUSA830157 | P00004 | 0.0 | 174724.59 | 2018-11-05 | 2018-11-05 16:21:04 | DEPARTMENT OF JUSTICE (DOJ) | ALL U NEED TEMPORARY SERVICES |\n",
    "9712 | GS07F0284N | DJJ15FUSA830157 | P00005 | 0.0 | 174724.59 | 2018-11-05 | 2018-11-05 16:26:26 | DEPARTMENT OF JUSTICE (DOJ) | ALL U NEED TEMPORARY SERVICES |\n",
    "9883 | GS07F0284N | DJJ15FUSA830157 | P00006 | 0.0 | 174724.59 | 2018-11-05 | 2018-11-05 16:30:53 | DEPARTMENT OF JUSTICE (DOJ) | ALL U NEED TEMPORARY SERVICES |\n",
    "\n",
    "one thing we might care to do is filter down this larger dataset of all the updated versions of records into only the most recent value (as defined by `modifcation_number` and `transaction_number`, or `last_modified_date`).\n",
    "\n",
    "we can do this with the following `python` code (assuming the dataframe has been loaded with `read_csv` as above):\n",
    "\n",
    "```python\n",
    "most_recent = df \\\n",
    "    .fillna({'parent_award_id': 'paid_nan', 'award_id_piid': 'aip_nan'}) \\\n",
    "    .sort_values(by=['parent_award_id', 'award_id_piid', 'last_modified_date']) \\\n",
    "    .groupby(['parent_award_id', 'award_id_piid']) \\\n",
    "    .last()\n",
    "```\n",
    "\n",
    "for the file downloaded via the link https://files.usaspending.gov/award_data_archive/2019_all_Contracts_Full_20181114.zip, we had\n",
    "\n",
    "+ 78,497 records in our *full*, unmodified `df` created with the `read_csv` function as above\n",
    "+ 70,702 most-recent records in our `most_recent` dataframe created as above\n",
    "\n",
    "\n",
    "## 4.5: what to submit\n",
    "\n",
    "this one's simple: take the `curl` or `wget` statement (line 1) and the `unzip` statement (line 2) that you used in part 2 and write those two lines to a file named `download_usaspending_sample.sh`\n",
    "\n",
    "\n",
    "##### upload `download_usaspending_sample.sh` to your `s3` homework submission bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 4: downloading a sample `usaspending` dataset <span style=\"color:red;font-weight:bold\">ANSWERS</span>\n",
    "\n",
    "the `sh` script is\n",
    "\n",
    "```sh\n",
    "wget https://files.usaspending.gov/award_data_archive/2019_all_Contracts_Full_20181114.zip\n",
    "unzip 2019_all_Contracts_Full_20181114.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid lightgrey;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 5: loading `usaspending` dataset into `hdfs`\n",
    "\n",
    "**inside your development environment you created above**, use the commands from the previous problem to download the 2019 contracts `zip` file and `unzip` to get a `csv` inside that development environment (e.g. within the container, vm, or `aws` `emr` instance). in that environment you may need to `yum` or `apt` install `wget`, `curl`, or `unzip`.\n",
    "\n",
    "let's load that `csv` file into `hdfs`. with one `hadoop` command, create a directory `/data` you can use to store data. with a second, `put` your `csv` into `hdfs` in your development environment at path `/data/usaspending.csv`\n",
    "\n",
    "write the statements you used to create the `/data` directory and `put` that file into `hdfs` to a file named `load_usaspending_to_hdfs.sh`\n",
    "\n",
    "##### upload `load_usaspending_to_hdfs.sh` to your `s3` submission bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 5: loading `usaspending` dataset into `hdfs` <span style=\"color:red;font-weight:bold\">ANSWERS</span>\n",
    "\n",
    "```sh\n",
    "hadoop fs -mkdir /data\n",
    "hadoop fs -put 2019_all_Contracts_Full_20181115_1.csv /data/usaspending.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid lightgrey;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 6: `pyspark` filter job for `usaspending` dataset\n",
    "\n",
    "let's duplicate our `pandas` filter work above with `pyspark`. we will jump through some hoops to use the `pyspark` `sql` `api` -- a time-tested way to groupby, aggregate, and window (what we need to do!)\n",
    "\n",
    "to do this we will use some libraries that are available in `spark` version 2+, so unfortunately our cheap development environments won't do. we'll spin up an `emr` instance instead! this is just a walkthrough, so you will only be executing commands -- this is to keep the cost of running your cluster low. **don't forget to terminate your cluster when you're done!!**\n",
    "\n",
    "\n",
    "## 6.1: launch an `emr` cluster\n",
    "\n",
    "spin up an `aws` `emr` cluster with `spark` installed. on the \"Create Cluster - Quick Options\" page, set the following\n",
    "\n",
    "+ whatever name you want\n",
    "+ select the `spark` software configuration option\n",
    "+ change the instance type to `m4.large`\n",
    "+ change the number of nodes from 3 (master and 2 core) down to 1 (just one master)\n",
    "+ pick an `ssh` key pair\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=1zk8-keZGm-H_vHTGqtY34cK_BJXAdXJu\"></div>\n",
    "\n",
    "\n",
    "## 6.2: getting the data (again)\n",
    "\n",
    "run the steps from the previous exercises to get a file in `hdfs` named `/data/usaspending.csv`\n",
    "\n",
    "\n",
    "## 6.3: launching `pyspark`\n",
    "\n",
    "make an `ssh` connection and launch `pyspark` from the command line. you will receive errors about logging; ignore them for now.\n",
    "\n",
    "\n",
    "## 6.4: loading a `csv`\n",
    "\n",
    "in `spark` v2 and beyond, you can load `csv`s from `hdfs` easily with the `spark.read.csv` function. do that!\n",
    "\n",
    "```python\n",
    "df = spark.read.csv('/data/usaspending.csv', header=True)\n",
    "```\n",
    "\n",
    "you can also count the records to make sure you have the same number as you saw in your local (`pandas`) investigations:\n",
    "\n",
    "```python\n",
    "# at time of writing, this returned 78,497\n",
    "df.count()\n",
    "```\n",
    "\n",
    "\n",
    "## 6.5: create a `Window` object\n",
    "\n",
    "the way that we will do our group by and sort is with a `Window` object (like the window operations in traditional `sql`. we do this in three steps:\n",
    "\n",
    "first, create a `spark` `Window` object:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window()\n",
    "```\n",
    "\n",
    "second, we will update that `Window` to partition all of our records by the keys we care about (\"partition\" is synonymous with \"group by\" here):\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = (Window()\n",
    "  .partitionBy('parent_award_id', 'award_id_piid'))\n",
    "```\n",
    "\n",
    "finally, our `Window` object which partitions records by our favored keys shoudl be *ordered*, so that we know our individual groups are sorted. we'll use the updated `Window` object's `.orderBy` method, and we will order based on the `col`umn `last_modified_date`, descending (so largest / most recent value on top)\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = (Window()\n",
    "  .partitionBy('parent_award_id', 'award_id_piid')\n",
    "  .orderBy(col('last_modified_date').desc())\n",
    "```\n",
    "\n",
    "\n",
    "## 6.6: using that `Window` object to calculate order within groups\n",
    "\n",
    "the `Window` object we just created is something that can be used to calculate aggregation functions within the defined windows. for example, we could calculate the `sum` or `average` of different values within the `groups` defined by the above `partitionBy` partitions.\n",
    "\n",
    "in this instance, we're looking to find the first record within each group (we have descending error), and we can do this by calculating the `row_number`. we will add this as a new column called \"rn\" to a dataframe, and save the result to a new dataframe of morst recent records\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "mr = (df\n",
    "  .withColumn(\"rn\", row_number().over(w)))\n",
    "```\n",
    "\n",
    "after adding that column, we will filter the overall dataset down to records where the `rn` value is 1 (the first row in the window group\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "mr = (df\n",
    "  .withColumn(\"rn\", row_number().over(w))\n",
    "  .where(col(\"rn\") == 1))\n",
    "```\n",
    "\n",
    "finally, from this filtered dataset we will select all columns\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "mr = (df\n",
    "  .withColumn(\"rn\", row_number().over(w))\n",
    "  .where(col(\"rn\") == 1)\n",
    "  .select(\"*\"))\n",
    "```\n",
    "\n",
    "you can also count the records to make sure you have the same number as you saw in your local (`pandas`) investigations:\n",
    "\n",
    "```python\n",
    "# at time of writing, this returned 70,702\n",
    "mr.count()\n",
    "```\n",
    "\n",
    "\n",
    "## 6.7: save the results\n",
    "\n",
    "now that we've filtered our dataset, we can immediately save the results for other downstream computation. each dataframe has a `.write.csv` method associated with it, and the first argument is the directory in `hdfs` in which we will save the output. *note: the output is not a single `csv`, but rather a directory containing a collection of files, each being the output of an chunk of records*\n",
    "\n",
    "```python\n",
    "out_dir = '/data/usaspending_most_recent/'\n",
    "mr.write.csv(out_dir)\n",
    "```\n",
    "\n",
    "\n",
    "## 6.8: the whole thing, together\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import row_number, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df = spark.read.csv('/data/usaspending.csv', header=True)\n",
    "df.count()\n",
    "\n",
    "w = (Window()\n",
    "  .partitionBy(\"parent_award_id\", \"award_id_piid\")\n",
    "  .orderBy(col(\"last_modified_date\").desc()))\n",
    "\n",
    "mr = (df\n",
    "  .withColumn(\"rn\", row_number().over(w))\n",
    "  .where(col(\"rn\") == 1)\n",
    "  .select(\"*\"))\n",
    "mr.count()\n",
    "\n",
    "out_dir = '/data/usaspending_most_recent/'\n",
    "mr.write.csv(out_dir)\n",
    "```\n",
    "\n",
    "\n",
    "## 6.9: check the output\n",
    "\n",
    "exit the `pyspark` session with\n",
    "\n",
    "```python\n",
    "exit()\n",
    "```\n",
    "\n",
    "back on the `emr` cluster command line, check the contents of the `mr.write.csv` command:\n",
    "\n",
    "```sh\n",
    "hadoop fs -ls /data/usaspending_most_recent\n",
    "```\n",
    "\n",
    "\n",
    "## 6.10: <span style=\"color:red;font-weight:bold\">SHUT OFF YOUR CLUSTER</span>\n",
    "\n",
    "don't forget to `terminate` your `emr` cluster!!\n",
    "\n",
    "\n",
    "##### there is nothing to submit here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 6: `pyspark` filter job for `usaspending` dataset <span style=\"color:red;font-weight:bold\">ANSWERS</span>\n",
    "\n",
    "it's just a walkthrough, so follow the instructions! the code here is\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import row_number, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df = spark.read.csv('/data/usaspending.csv', header=True)\n",
    "df.count()\n",
    "\n",
    "w = (Window()\n",
    "  .partitionBy(\"parent_award_id\", \"award_id_piid\")\n",
    "  .orderBy(col(\"last_modified_date\").desc()))\n",
    "\n",
    "mr = (df\n",
    "  .withColumn(\"rn\", row_number().over(w))\n",
    "  .where(col(\"rn\") == 1)\n",
    "  .select(\"*\"))\n",
    "mr.count()\n",
    "\n",
    "out_dir = '/data/usaspending_most_recent/'\n",
    "mr.write.csv(out_dir)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid lightgrey;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 7: resolving an issue with a `commit` to a `branch`\n",
    "\n",
    "recall from a previous assignment that `github` -- *not* `git` itself -- allows you to create issues to track bugs and feature requests, and to close issues with commits. previously we closed issues by pushing changes to `master`; now we will resolve them by pushing changes to a `branch` and using the `github` web interface to close them via a `pull request`.\n",
    "\n",
    "I have already added an issue to your repositories requesting a simple change be made. the issue's title is **die, 511 homeworks!**, and the goal is to create a new file called `status_report` which logs that we've finished our homework for MATH 511.\n",
    "\n",
    "\n",
    "## 7.1: viewing and assigning the issue\n",
    "\n",
    "log in to `github` and click on the \"issues\" tab, and open the issue I created for you. in particular, I want you to **assign** it to yourself -- click on the \"assign yourself\" link on the issues page.\n",
    "\n",
    "also, **make note of the issue number!**. you will need to include that number in your `commit` `message`\n",
    "\n",
    "\n",
    "## 7.2: creating a `branch`\n",
    "\n",
    "in your local repo, create a new `branch` named `statusreport` and check that `branch` out\n",
    "\n",
    "\n",
    "## 7.3: adding a status report\n",
    "\n",
    "create a new file named `status_report.txt` with only one line: `homework is finished`.\n",
    "\n",
    "`add` that file to tracking and `commit` it to your `statusreport` `branch`. for your `commit` message, use\n",
    "\n",
    "```\n",
    "# replace #N with your issue number!!\n",
    "status report: initial commit, fixes #N\n",
    "```\n",
    "\n",
    "with `#N` above replaced by the issue number you have in `github`.\n",
    "\n",
    "\n",
    "`push` this `commit` to `github`. remember: you don't push `branch`es to `origin master`, you push them to `origin [branch name]`!)\n",
    "\n",
    "\n",
    "## 7.4: check out the new `pull request` on `github`\n",
    "\n",
    "log in to `github`. `github` noticed that the `commit` you just pushed has a reference to an issue (`\"fixes #5\"`) and provides you with a way to view the newly-created `pull request`\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=14RoWxzI2q5nUwRquQdrTxw9nyUiI7LCt\" width=\"700px\"></div>\n",
    "\n",
    "click on the \"compare & pull request\" button to go to the \"Open a pull request\" page that `github` has generated for us:\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=1WQRvvuQlAn5bnYKcFne_Rdu1hO-x3kcn\" width=\"700px\"></div>\n",
    "\n",
    "this page is a form that will allow us to create a \"pull request\" -- a `github` concept that is, basically, a web-based merge of `branch`es resolving `issue`s. click the \"create pull request\" button to create an official pull request.\n",
    "\n",
    "the page we are looking at will then update to a pull request. `github` will first calculate whether or not the changes in this `branch` can be directly merged into the `master` branch. if so, it will give us the ability to do the `merge` through the web console rather than the command line (pretty cool!)\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=1BEUglXC0bjFLlCvziqNYTwNSapjmiq1f\" width=\"700px\"></div>\n",
    "\n",
    "\n",
    "## 7.5: merge!\n",
    "\n",
    "click that \"Merge pull request\" button and let 'er rip! the result is a successful pull request, and the webpage will update inplace:\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=1KKGex6RXsED75XWsNrO5q6RXydnjVs-9\" width=\"700px\"></div>\n",
    "\n",
    "\n",
    "## 7.6: view the new `merge` `commit` on the command line\n",
    "\n",
    "the changes we just made to `master` in that pull request were all done on `github`. our `remote` knwos about them, but our `local` doesn't yet!\n",
    "\n",
    "back in your local repo, `checkout` `master` and `pull` the `remote` changes. you should see the merge commit we just created completely online on `github` merging your `statusreport` `branch` with `master`, like this:\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=1GDsYUK8vQg-c_UE9MAgbPj7C-y2_jA2s\" width=\"700px\"></div>\n",
    "\n",
    "\n",
    "##### submission will be verified via `github`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 7: resolving an issue with a `commit` to a `branch` <span style=\"color:red;font-weight:bold\">ANSWERS</span>\n",
    "\n",
    "locally,\n",
    "\n",
    "```sh\n",
    "# the below can be combined into one command:\n",
    "#    git checkout -B statusreport\n",
    "git branch statusreport\n",
    "git checkout statusreport\n",
    "\n",
    "echo \"homework is finished\" > statusreport.txt\n",
    "\n",
    "git add statusreport.txt\n",
    "git commit -m 'status report: initial commit, fixes #N'\n",
    "git push origin statusreport\n",
    "```\n",
    "\n",
    "then, go through the items listed above on `github`. once the `pull request` has been `merge`d into `master`, back in your local repo do\n",
    "\n",
    "```sh\n",
    "git checkout master\n",
    "git pull\n",
    "git gr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid lightgrey;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 8: fill out a year-end course survey\n",
    "\n",
    "I'd like your feedback on the course -- please fill out the form at https://goo.gl/forms/3Psw2IRUYs1aoO8c2. this is 100% anonymous and not mandatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 8: fill out a year-end course survey <span style=\"color:red;font-weight:bold\">ANSWERS</span>\n",
    "\n",
    "no answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid lightgrey;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
