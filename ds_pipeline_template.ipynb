{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# example data science pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's run through a couple steps to fit several models to the adult income dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line configures matplotlib (the backbone of the pandas\n",
    "# plotting functions) to render the graphs it makes in the \n",
    "# notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# for these \"import ... as ..\", the alias terms (phrases after \"as\")\n",
    "# are simply conventions. You will usually see stack overflow code\n",
    "# referencing these aliases\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import sklearn.externals.joblib\n",
    "import sklearn.feature_selection\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import sklearn.neural_network\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "\n",
    "# this step invokes seaborn one time to make the default plot\n",
    "# configurations of matplotlib less heinous\n",
    "sns.set()\n",
    "\n",
    "# this command informs the plotly module that you are connected\n",
    "# to the internet but wish to run in \"offline\" mode (that is,\n",
    "# graph things like a normal plotting library instead of sending\n",
    "# everything off to plotly HQ)\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's go download a relatively large dataset that is available as part of the [UCI machine learning repository](http://archive.ics.uci.edu/ml/index.php). I've chosen the [Adult](http://archive.ics.uci.edu/ml/datasets/Adult) dataset \n",
    "\n",
    "### keeping things simple\n",
    "\n",
    "we *could* use the requests library to download and parse the column names (available [here](http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names)), but instead I've just hard-coded them below.\n",
    "\n",
    "also, we *could* use the pre-segregated train and test data sets as our train and test, but that would involve some data munging and cleaning that is a bit of a mess, and also results in enough data points in our final plots that we'd have to change some annoying configuration variables. instead, let's pull only the smaller training dataset, and use the `scikit-learn` train / test split function to create a test dataset of our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week',\n",
    "    'native-country',\n",
    "    'target'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',\n",
    "    names=columns,\n",
    "    delimiter=', ',\n",
    "    index_col=False,\n",
    "    engine='python'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's print the first 5 records of that dataset `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- #\n",
    "# FILL ME IN !!!! #\n",
    "# --------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ugh. those column names... those dashes... abominations.\n",
    "\n",
    "let's clean that up. I can clean up one using the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'capital-gain'\n",
    "col.replace('-', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's use a list comprehension to clean up the elements of `df.columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns = [\n",
    "# --------------- #\n",
    "# FILL ME IN !!!! #\n",
    "# --------------- #\n",
    "#]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now, the first 5 records again, to see that the column names have changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- #\n",
    "# FILL ME IN !!!! #\n",
    "# --------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and it'd be good to know the shape of this dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we go along, to verify that everything has worked as expected, we will use `assert` statements. `assert` is a `python` statement which will \"do nothing\" if the thing you are `assert`ing is `True`, and will raise an `AssertionError` if it is not. this is a way of silently verifying that code you've writen is behaving as expected. try the following, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1 == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's verify that the dataframe we've constructed so far is the right shape before moving on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.shape == (32561, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the rest of this notebook, **if you fail an `assert` statement, something has gone wrong**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's put together a sequence of pre-processing operations we wish to do to our test and train datatset. Since we're going to do it to both test and train, it'd be good to build up a function along the way (so we don't have to repeat every command twice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropping some columns\n",
    "\n",
    "let's skip some of the hard work and just know, ahead of time, that the column `fnlwgt` and `education_num` should be dropped. Why?\n",
    "\n",
    "1. `fnlwgt` is a weighting for demographic sampling, and is an estimate of how many people fall into the given category. we're not going to use this weighting, so let's get rid of it.\n",
    "2. `education_num` is a numerical representation of the values in the `education` column. You could argue that you should keep this numeric column and drop the `education` column, or convert `education` into a dummy column and drop `education_num`. we'll do the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['fnlwgt', 'education_num'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.shape[1] == 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting categorical factors to numerical dummy factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's convert the categorical columns in this dataset to dummy variables, and then extract the numerical values into an `X` and `Y` dataset\n",
    "\n",
    "first, let's look at the number of values for each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print('{}: {}'.format(col, df[col].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "investigation of the number of valid elements (as well as a measure of common sense) leads to the following list of categorical values which should be converted into dummy variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummycols = [\n",
    "    'workclass',\n",
    "    'education',\n",
    "    'marital_status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'native_country'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `target` column, though categorical, is left out because we're about to use the above list to convert our categorical *predictors* into dummy variables. `pandas` provides a simple function for doing just that:\n",
    "\n",
    "[`pd.get_dummies`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html). look at the documentation quick to figure out how it will work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use this function to add dummy features. in particular, set the following parameters:\n",
    "\n",
    "1. `data`: this is the base dataframe from which we want to build *some* dummy variables\n",
    "1. `dummy_na`: actually, *none* of our categories have `nan` values. you should verify this, but you can take my word for it and *not* create a dummy column for `nan` values\n",
    "1. `columns`: use the list of dummy columns we generated just above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(\n",
    "    # --------------- #\n",
    "    # FILL ME IN !!!! #\n",
    "    # --------------- #\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.shape[1] == 107"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fixing column names again\n",
    "\n",
    "some of those dummy variable names are pretty gnarly (dashes and capital letters abound!), so let's fix the column names one more time. we could fix a single column name via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'native_country_Puerto-Rico'\n",
    "col.lower().replace('-', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns = [\n",
    "# --------------- #\n",
    "# FILL ME IN !!!! #\n",
    "# --------------- #\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert {\n",
    "    _ for _ in df.columns if _.startswith('sex')\n",
    "} == {'sex_female', 'sex_male'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fixing the target categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "currently, our target looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's replace those string values with numerical targets.\n",
    "\n",
    "there are two ways we could do this. first, we could convert this factor into a *category*, and then use the integer values (called *codes*) that are assigned to the two types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target.astype('category').cat.codes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the second way we could do this is with a simple boolean expression, converted to an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.target == '>50K').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.target == '>50K').astype(int).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "either would work. I'll go with the `bool` version for this step\n",
    "\n",
    "in `pandas`, whenever you *create* or *re-assign* features, you should use the `.loc` or `.iloc` indexers (not the `.` or `[...]` notations, as might feel more natural):\n",
    "\n",
    "```python\n",
    "# one new or re-assigned feature\n",
    "df.loc[:, 'colname'] = newvalues\n",
    "\n",
    "# several new or re-assigned features\n",
    "df.loc[:, ['col1', 'col2', ...]] = newvalues\n",
    "```\n",
    "\n",
    "in the above, the items inside the `[...]` are the row index slice statement (a \"`:`\" character means \"all rows\"), and the item after the comma is the column index slice statement (a column name as a string, or a list of multiple columns).\n",
    "\n",
    "the item on the left will have a certain number of rows and a certain number of columns; the right hand side must be of the same size.\n",
    "\n",
    "we'll use the `loc` indexer here (we are *replacing* `target` values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'target'] = (df.target == '>50K').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.target.unique().tolist() == [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropping non-numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's get rid of the text features (most of which we just converted into dummy variables). this is straight-forward using the *hidden* dataframe class member function (items with one leading `_` are called \"hidden\" member functions) `_get_numeric_data`, which restricts a dataframe to only those columns with a *numeric* `dtype` (a `numpy` concept representing the type of data stored in a column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df._get_numeric_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.shape[1] == 107"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `log`-transform of monetary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have some monetary information, so it'd be good to `log` normalize those. Let's also normalize all numerical features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moneycols = [\n",
    "    'capital_gain',\n",
    "    'capital_loss'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to show these distributions have long tails and problematic distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[moneycols].hist(figsize=(14, 6), bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can easily calculate the `log(1 + x)` value for those columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log1p = np.log1p(df[moneycols])\n",
    "log1p.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what about the distributions under this transform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log1p.hist(figsize=(14, 6), bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's use the `loc` indexer again, this time to *replace* the values of our `moneycols` with these `log`-transformed features.\n",
    "\n",
    "in this case, we want to replace several columns, so we will write\n",
    "\n",
    "```python\n",
    "df.loc[:, listOfColumnNames] = newValuesForThoseColumns\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- #\n",
    "# FILL ME IN !!!! #\n",
    "# --------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.capital_gain.max() < 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standardizing numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's go ahead and do the simplest thing -- standardizing *every* non-target variable. this is a little meaningless for dummy variables, but hey -- it's just an excercise, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nottarget = [col for col in df.columns if col != 'target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can scale items in a dataframe using the `sklearn.preprocessing.scale` function (note: this returns a `numpy` array without column names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.preprocessing.scale(df[nottarget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the column list `nottarget` we just created, the `.loc` dataframe member function, and the `sklearn.preprocessing.scale` function to replace all non-target numeric values with their scaled values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- #\n",
    "# FILL ME IN !!!! #\n",
    "# --------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert -2 < df.age.min() < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0 < df.age.max() < 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to have some completely unseen data for final model validation, it's a good idea to hold out about 20 percent of your data. We can use the `sklearn.model_selection.train_test_split` function to split a single dataframe of labeled cases into two sets (train and test) of predictors and targets (`X` and `Y`), while keeping the prevalance of our target variable fixed between our training and testing `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.model_selection.train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain, dftest = sklearn.model_selection.train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=1337,\n",
    "    stratify=df.target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dftrain.shape[0] == 26048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the prevalence of the target (`target`) in each dataset is approximately equivalent\n",
    "\n",
    "*hint: there are several ways you could calculate this, but you should look to the `value_counts` function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- #\n",
    "# FILL ME IN !!!! #\n",
    "# --------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- #\n",
    "# FILL ME IN !!!! #\n",
    "# --------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0.74 <= (dftest.target == 0).mean() <= 0.76"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `x` and `y`, at last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's split our dataframes into `x` and `y` values for easier use in the `sklearn` world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = dftrain[nottarget].values\n",
    "ytrain = dftrain['target'].values\n",
    "xtest = dftest[nottarget].values\n",
    "ytest = dftest['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert xtrain.shape == (26048, 106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ytrain.shape == (26048,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fitting a data science pipeline\n",
    "\n",
    "the hard part is over! let's use `scikit-learn` to create a pipeline that combines feature selection and a modelling approach, and then evaluate the results of our model against the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature selection\n",
    "\n",
    "let's try recursive feature elimination with random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE with random forests\n",
    "rf = sklearn.ensemble.RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    n_jobs=-1,\n",
    "    random_state=1337\n",
    ")\n",
    "rfe = sklearn.feature_selection.RFE(\n",
    "    estimator=rf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modeler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try a random forest, a logistic regression, and a neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrf = sklearn.ensemble.RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    n_jobs=-1,\n",
    "    random_state=1337,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we could call the above items sequentially:\n",
    "\n",
    "```python\n",
    "xfs = rfe.fit_transform(xtrain, ytrain)\n",
    "mrf.fit(xtrain, ytrain)\n",
    "```\n",
    "\n",
    "and that'd be fine. however, `scikit-learn` has a concept of *pipelines*, sequences of events that all expose these `fit`, `transform`, and `fit_transform` methods. it allows you to chain these items together in a sort of linux-esque sequence of piped commands (hence, pipeline).\n",
    "\n",
    "let's create a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = sklearn.pipeline.Pipeline(\n",
    "    steps=[\n",
    "        # a sequence of name, transformer objects\n",
    "        ('rfe', rfe),\n",
    "        ('random_forest', mrf)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "fitting our model then is a simple call of the pipeline's `fit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature importance\n",
    "\n",
    "our pipeline has a feature selection element we can access via the `named_steps` member variable.\n",
    "\n",
    "that fitted feature selection object has a few variables of interest:\n",
    "\n",
    "1. `support`, accessed via `get_support()`: a boolean indicating whether or not a feature should be kep\n",
    "2. `importance`: the average feature importance of a given feature across all the trees in our random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = pipeline.named_steps['rfe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsupport = pd.DataFrame({\n",
    "    'feature': nottarget,\n",
    "    'support': fs.get_support()\n",
    "})\n",
    "dfsupport.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only have feature importance for records where `support` is true\n",
    "dfsupport.loc[\n",
    "    dfsupport.support, 'importance'\n",
    "] = fs.estimator_.feature_importances_\n",
    "dfsupport.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's look at the five most important features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsupport = dfsupport.sort_values(by='importance', ascending=False)\n",
    "dfsupport.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, let's use `plotly`'s `Bar` element to create a **horizontal** bar chart. I'll fill in the majority of the items here, but you will need to figure out how to construct the actual bar chart data element.\n",
    "\n",
    "this might be helpful: https://plot.ly/python/horizontal-bar-charts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the features which weren't chosen, and invert the sort\n",
    "# order (plotly adds bars in this \"top to bottom\" way for\n",
    "# horizontal bar charts)\n",
    "nonzero = dfsupport[dfsupport.support].sort_values(by='importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use some features of nonzero (created above) \n",
    "# to fill in the elements of your bar chart.\n",
    "# also, make sure the bar chart is horizontal!\n",
    "barchart = go.Bar(\n",
    "    # --------------- #\n",
    "    # FILL ME IN !!!! #\n",
    "    # --------------- #\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert barchart.orientation == 'h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [barchart]\n",
    "\n",
    "layout = go.Layout(\n",
    "    # tall enough to include every feature we selected\n",
    "    height=1200,\n",
    "    # enough space to display the full feature name\n",
    "    margin=go.layout.Margin(l=250),\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also easily make predictions with the fit pipeline object -- this will take raw input data, apply feature selection, and score the records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = pipeline.predict_proba(xtest)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the two columns in `ypred` are the predicted probability of the classes 0 and 1 for the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thus the probability of having a target value of 1 (equivalently: having a salary over $50K) is the second column\n",
    "\n",
    "let's combine the predicted probabilities for our test set with the known labeled ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpred = pd.DataFrame({\n",
    "    'ytest': ytest,\n",
    "    'ypred': ypred[:, 1]\n",
    "})\n",
    "dfpred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's use plotly to plot the cumulative captured response on the held out test data from the original dataframe. to do this, we will need to pick out the now-trained pipeline corresponding to our best run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpred = dfpred.sort_values(by='ypred', ascending=False)\n",
    "ntargets = dfpred.ytest.sum()\n",
    "dfpred.loc[:, 'pct_captured'] = dfpred.ytest.cumsum() / ntargets\n",
    "\n",
    "xarr = np.array(range(dfpred.shape[0]))\n",
    "yperf = np.ones(xarr.shape)\n",
    "yperf[:ntargets] = np.linspace(0, 1, ntargets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    # our capture rate\n",
    "    go.Scatter(\n",
    "        x=xarr,\n",
    "        y=dfpred.pct_captured,\n",
    "        mode='lines',\n",
    "        line={'width': 2},\n",
    "        name='our prediction'\n",
    "    ),\n",
    "    # random choice\n",
    "    go.Scatter(\n",
    "        x=xarr,\n",
    "        y=xarr / xarr.max(),\n",
    "        mode='lines',\n",
    "        line={\n",
    "            'dash': 'dash',\n",
    "            'color': 'black',\n",
    "            'width': 1,\n",
    "        },\n",
    "        name='random'\n",
    "    ),\n",
    "    # perfect\n",
    "    go.Scatter(\n",
    "        x=xarr,\n",
    "        y=yperf,\n",
    "        mode='lines',\n",
    "        line={\n",
    "            'dash': 'dot',\n",
    "            'color': 'black',\n",
    "            'width': 1,\n",
    "        },\n",
    "        name='perfect'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a layout with axes labels and title\n",
    "layout = go.Layout(\n",
    "    title='cumulative captured response',\n",
    "    xaxis={'title': 'number of records recommend and investigated'},\n",
    "    yaxis={'title': 'fraction of all true cases obtained'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, create the `plotly go.Figure` object using the `data` and `layout` elements above, and create the `offline plot` (note: we did exactly this for the horizontal bar plot above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- #\n",
    "# FILL ME IN !!!! #\n",
    "# --------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that is *pretty good*.\n",
    "\n",
    "maybe *too pretty good*..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# you're done!\n",
    "\n",
    "You can feel free to submit the homework if you've gotten to this point. However, what follows is a bit of an advanced digression into training multiple models, selecting the best one based on cross-validation, and evaluating that (hopefully better) model on the test data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **advanced**: cross-validation of multiple pipelines for model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we chose an arbitrary feature selection method and modelling approach above and had pretty good results from it -- lucky!\n",
    "\n",
    "in practice, it would be better to try several different feature selection and modelling methods, and to try and find some test-agnostic way of selecting the best among them. a common approach is to create many different pipelines and for each pipeline evaluate a given metric under cross validation. The model with the best cross-validated metric score is then selected, and the final evaluation is then performed against the held out (test) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation\n",
    "\n",
    "we will need to do some cross validation for\n",
    "\n",
    "1. grid searches for parameters, and\n",
    "2. feature or model selection by metric scores on cross-validated samples\n",
    "\n",
    "to accomplish this, we'll use the `sklearn.model_selection.StratifiedShuffleSplit`, an implementation of a fixed bootstrap cross validation selector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = sklearn.model_selection.StratifiedShuffleSplit(\n",
    "    n_splits=10,\n",
    "    test_size=0.2,\n",
    "    random_state=1337\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature selectors\n",
    "\n",
    "let's try recursive feature selection with random forests and lasso (done using the logistic regression cross validation model with an L1 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE with random forests\n",
    "rf = sklearn.ensemble.RandomForestClassifier(n_jobs=-1, random_state=1337)\n",
    "rfe = sklearn.feature_selection.RFE(\n",
    "    estimator=rf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso\n",
    "lr = sklearn.linear_model.LogisticRegression(\n",
    "    C=.1,\n",
    "    penalty='l1',\n",
    "    solver='saga',\n",
    "    n_jobs=-1,\n",
    "    random_state=1337,\n",
    "    max_iter=250\n",
    ")\n",
    "lasso = sklearn.feature_selection.SelectFromModel(estimator=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modelers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try a random forest, a logistic regression, and a neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrf = sklearn.ensemble.RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    n_jobs=-1,\n",
    "    random_state=1337,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mLogRegCv = sklearn.linear_model.LogisticRegressionCV(\n",
    "    Cs=np.logspace(-3, 3, 7),\n",
    "    cv=cv,\n",
    "    scoring='neg_log_loss',\n",
    "    n_jobs=-1,\n",
    "    max_iter=500,\n",
    "    random_state=1337,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mMlp = sklearn.neural_network.MLPClassifier(\n",
    "    hidden_layer_sizes=(25,10),\n",
    "    activation='logistic',\n",
    "    max_iter=500,\n",
    "    random_state=1337,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each combo of feature selection method and model, let's create a pipeline. these pipelines can be passed to a model selection method to help us choose the best model among all combinations, just as we did above.\n",
    "\n",
    "we don't *need* names for each step (they will be created as the class names of the objects passed in), but it's nice. we can use the `zip` function to combine lists of names and lists of objects. this means we can create our entire collection of pipelines using a list comprehension and some iterator functions.\n",
    "\n",
    "given that we will have two feature selection methods and three models, that's a combination of 6 pipelines. It'd be nice to not have to perform feature selection 3 times when one is sufficient -- we can accomplish this using a cached memory feature built in to `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "cachedir = tempfile.mkdtemp()\n",
    "memory = sklearn.externals.joblib.Memory(location=cachedir)\n",
    "print('cachedir was {}'.format(cachedir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [\n",
    "    sklearn.pipeline.Pipeline(\n",
    "        steps=[\n",
    "            (fsname, fs),\n",
    "            (modelname, model)\n",
    "        ],\n",
    "        memory=memory\n",
    "    )\n",
    "    for (fsname, fs) in [\n",
    "        ('lasso', lasso),\n",
    "        ('rfe', rfe)\n",
    "    ]\n",
    "    for (modelname, model) in [\n",
    "        ('random_forest', mrf),\n",
    "        ('logistic', mLogRegCv), \n",
    "        ('neural_net', mMlp)\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selecting model via cross validation\n",
    "\n",
    "I will collect the scores from each cross validation loop into a dataframe. we can then later group that data frame by model and feature selection type (columns `m` and `fs` below, resp.) to get average and standard deviation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfscores = pd.DataFrame()\n",
    "\n",
    "for p in pipelines:\n",
    "    fsname = p.steps[0][0]\n",
    "    mname = p.steps[1][0]\n",
    "    print('{} - {}'.format(fsname, mname))\n",
    "    score = sklearn.model_selection.cross_validate(\n",
    "        estimator=p,\n",
    "        X=xtrain,\n",
    "        y=ytrain,\n",
    "        scoring=('accuracy', 'neg_log_loss'),\n",
    "        cv=cv,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    score['fs'] = fsname\n",
    "    score['m'] = mname\n",
    "    dfscoresnow = pd.DataFrame(score)\n",
    "    \n",
    "    dfscores = dfscores.append(dfscoresnow, ignore_index=True)\n",
    "    \n",
    "dfscores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfscores.groupby(['fs', 'm']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfscores.groupby(['fs', 'm']).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppose we want to choose the item with the best (here: largest) negative log loss on test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, m = dfscores.groupby(['fs', 'm']).mean().test_neg_log_loss.idxmax()\n",
    "\n",
    "print(fs)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's use plotly to plot the cumulative captured response on the held out test data from the original dataframe. \n",
    "\n",
    "to do this, we will need to pick out the now-trained pipeline corresponding to our best run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [p for p in pipelines if fs in p.named_steps and m in p.named_steps][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the pipeline has *not* been fit on the full training data yet, just bootstrapped sub-samples of the training set. let's train it on the full model, and use that trained model to predict on our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.fit(xtrain, ytrain)\n",
    "ypred = p.predict_proba(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpred = pd.DataFrame({\n",
    "    'ytest': ytest,\n",
    "    'ypred': ypred[:, 1]\n",
    "})\n",
    "\n",
    "dfpred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpred = dfpred.sort_values(by='ypred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpred = dfpred.sort_values(by='ypred', ascending=False)\n",
    "ntargets = dfpred.ytest.sum()\n",
    "dfpred.loc[:, 'pct_captured'] = dfpred.ytest.cumsum() / ntargets\n",
    "\n",
    "xarr = np.array(range(dfpred.shape[0]))\n",
    "yperf = np.ones(xarr.shape)\n",
    "yperf[:ntargets] = np.linspace(0, 1, ntargets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    # our capture rate\n",
    "    go.Scatter(\n",
    "        x=xarr,\n",
    "        y=dfpred.pct_captured,\n",
    "        mode='lines',\n",
    "        line={'width': 2},\n",
    "        name='our prediction'\n",
    "    ),\n",
    "    # random choice\n",
    "    go.Scatter(\n",
    "        x=xarr,\n",
    "        y=xarr / xarr.max(),\n",
    "        mode='lines',\n",
    "        line={\n",
    "            'dash': 'dash',\n",
    "            'color': 'black',\n",
    "            'width': 1,\n",
    "        },\n",
    "        name='random'\n",
    "    ),\n",
    "    # perfect\n",
    "    go.Scatter(\n",
    "        x=xarr,\n",
    "        y=yperf,\n",
    "        mode='lines',\n",
    "        line={\n",
    "            'dash': 'dot',\n",
    "            'color': 'black',\n",
    "            'width': 1,\n",
    "        },\n",
    "        name='perfect'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a layout with axes labels and title\n",
    "layout = go.Layout(\n",
    "    title='cumulative captured response',\n",
    "    xaxis={'title': 'number of records recommend and investigated'},\n",
    "    yaxis={'title': 'fraction of all true cases obtained'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a figure to join the above\n",
    "fig = go.Figure(\n",
    "    data=data,\n",
    "    layout=layout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean up after yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(cachedir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
