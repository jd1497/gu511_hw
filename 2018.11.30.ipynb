{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises due by EOD 2018.11.30 (TWO weeks from now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this homework assignment we will read ahead in lectures, work with the `rds` and `redshift` databases, and handle some `git` issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## method of delivery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as mentioned in our first lecture, the method of delivery may change from assignment to assignment. we will include this section in every assignment to provide an overview of how we expect homework results to be submitted, and to provide background notes or explanations for \"new\" delivery concepts or methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this week you will be submitting the results of your homework via an email to **BOTH** Zach (rzl5@georgetown.edu) and Carlos (chb49@georgetown.edu) titled \"2018.11.30 answers\", uploading files to an `s3`, and commits to your `gu511_git_hw` on `github`\n",
    "\n",
    "summary:\n",
    "\n",
    "| exercise | deliverable | method of delivery |\n",
    "|----------|-------------|--------------------|\n",
    "| 1 | none | none |\n",
    "| 2 | none | none |\n",
    "| 3 | a record in a table | `insert` the record using `psql` |\n",
    "| 4 | a complete `dbconnections.py` | uploaded to your `s3` homework submission bucket from last week |\n",
    "| 5 | none | none |\n",
    "| 6 | none | none |\n",
    "| 7 | a `bulk_insert_times.csv` collection of time values | uploaded to your `s3` homework submission bucket from last week |\n",
    "| 8 | none | none |\n",
    "| 9 | a completed `create_trainpositions.sql` file | uploaded to your `s3` homework submission bucket from last week |\n",
    "| 10 | a completed `copy_trainpositions.sql` file | uploaded to your `s3` homework submission bucket from last week |\n",
    "| 11 | a completed `wmata_queries.sql` file | uploaded to your `s3` homework submission bucket from last week |\n",
    "| 12 | a completed `redshift.py` file | uploaded to your `s3` homework submission bucket from last week |\n",
    "| 13 | a `commit` which resolves an issue | pushed to your shared `github` repository |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 1: read the `dynamodb` and `neo4j` lectures\n",
    "\n",
    "read the lectures\n",
    "\n",
    "+ `012_dbs_3_dynamodb.ipynb`\n",
    "+ `012_dbs_4_neo4j.ipynb`\n",
    "\n",
    "these lectures will not be covered in class and you will not be directly tested on the material covered therein, but you should know the purpose and use case of those services as well \n",
    "\n",
    "\n",
    "##### there is nothing to submit for this question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 2: read the `redshift` lecture\n",
    "\n",
    "read the lectures `012_dbs_5_redshift.ipynb`. this lecture **will** be covered (though briefly and in retrospect) in class (2018.11.29) and questions in this homework **will** be covering this topic\n",
    "\n",
    "##### there is nothing to submit for this question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 3: connect and write to a `postgres` database\n",
    "\n",
    "we have set up a shared `postgres` database with the following connection details:\n",
    "\n",
    "| parameter  | value                                                       |\n",
    "|------------|-------------------------------------------------------------|\n",
    "| `host`     | `gu511.cdmknaubrmaw.us-east-1.rds.amazonaws.com` |\n",
    "| `port`     | `5432` |\n",
    "| `username` | `gu511` |\n",
    "| `db`       | `gu511db` |\n",
    "| `password` | emailed to you |\n",
    "\n",
    "we would like for you to install the `psql` client (not the `postgres` database) on your `ec2 ubuntu` instances, use it to connect to this shared database, and `insert` a value into a shared table.\n",
    "\n",
    "\n",
    "## install `psql`\n",
    "\n",
    "on your `ec2 ubuntu` instances, install `psql` using the command\n",
    "\n",
    "```bash\n",
    "sudo apt install postgresql-client\n",
    "```\n",
    "\n",
    "you can verify that your installation worked be checking\n",
    "\n",
    "```bash\n",
    "which psql\n",
    "```\n",
    "\n",
    "and receiving an output that is the file path to the `psql` executible\n",
    "\n",
    "\n",
    "## connect to the database\n",
    "\n",
    "using the values we provided in the table above, the course notes, and `man psql`, figure out how to connect to our shared database with the `psql` command\n",
    "\n",
    "\n",
    "## `insert` a new record\n",
    "\n",
    "the following command will insert a record into a shared database we've created called `gu511_shared_table`. the table has two columns:\n",
    "\n",
    "1. `guid`: your georgetown user id\n",
    "2. `message`: a test message to verify you were able to successfully insert your record\n",
    "\n",
    "once you've successfully connect to the `postgres` database, insert a record with the following query (replacing `MY_GUID` and `MY_MESSAGE` with real values, of course):\n",
    "\n",
    "```sql\n",
    "INSERT INTO gu511_shared_table (guid, message) VALUES ('MY_GUID', 'MY_MESSAGE');\n",
    "```\n",
    "\n",
    "*note*: the single quotes in `'MY_GUID', 'MY_MESSAGE'` are required. they indicate that the values being passed in are strings, which is the datatype of those two coumns in our table\n",
    "\n",
    "\n",
    "##### the submission is the record in the shared database -- we will check results by running `SELECT * FROM gu511_shared_table;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 4: `boto3` and the `rds` service\n",
    "\n",
    "remember: the `rds` service is a totally separate concept from the database that is implemented with `rds`. think of it as the chef: it collects the ingredients (one of which is `postgres`) and bakes you a `postgres` cake. we're going to talk to the chef right now to figure out some of the configuration options for our database, then use those values to connect to the *database* (not the `rds` service).\n",
    "\n",
    "download the skeleton `python` script here: https://s3.amazonaws.com/shared.rzl.gu511.com/dbconnections.py\n",
    "\n",
    "fill in the missing pieces to construct a function which can take an `rds` database id (that is, the name that appears in the \"Database ID\" column on the `rds` \"instances\" page, e.g. `rzl-tue-test` or `gu511`) and create a `psycopg2` or `sqlalchemy` connection object from the returned information.\n",
    "\n",
    "in order to execute the `boto3` requests for this code, you will need the following:\n",
    "\n",
    "1. an `rds` `postgres` database up and running\n",
    "1. an `aws config` profile or an `ec2` `iam` `role` which has `AmazonRDSReadOnlyAccess` or `AmazonRDSFullAccess` policies attached to it\n",
    "1. a `python` 3 environment with `psycopg2` and `sqlalchemy` installed.\n",
    "\n",
    "if the above is done, you can test this file by running the following (the part in brackets is optional; `profile_name` can be left empty if you have a `default` profile or an `ec2` `iam` `role`)\n",
    "\n",
    "```bash\n",
    "python dbconnections.py --dbid YOUR_RDS_DB_ID [--profile_name YOU_AWS_CONFIGURE_PROFILE]\n",
    "```\n",
    "\n",
    "we will test your result by running your file with our own `dbid` and `profile name`\n",
    "\n",
    "\n",
    "##### upload your modified `dbconnections.py` file to the `s3` homework bucket you created last week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 5: creating an `rds` postgres instance\n",
    "\n",
    "create your own `postgres` `rds` instance using the `aws` `rds` service with the following properties (note: on the \"Select Engine\" screen, if you click the \"Only enable options elegible for RDS free usage tier\", most of these options will be filled in for you):\n",
    "\n",
    "1. engine: `postgresql`\n",
    "2. engine version: `PostgreSQL 10.4`\n",
    "3. instance class: `db.t2.micro`\n",
    "4. storage type: SSD\n",
    "5. storage: 20 GB\n",
    "\n",
    "be sure to capture the hostname, port, database name, master user name, and master user password.\n",
    "\n",
    "##### there is nothing to submit for this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 6: installing `postgres` once for yourself\n",
    "\n",
    "\n",
    "the goal of this exercise is to replicate what `rds` does for us: create an `ec2` instance and install the `postgres` service. let's start by creating that server.\n",
    "\n",
    "\n",
    "## 6.1: create the `ec2` server\n",
    "\n",
    "first, using the `aws` `ec2` service, create a new `ec2` instance that is as close as possible to the underlying `ec2` instance of the `postgres` database instance we created in the previous problem  (that is, it should have the following properties):\n",
    "    \n",
    "+ `ami`: Amazon Linux AMI 2018.03.0 64 bit (free tier)\n",
    "+ instance type: `t2.micro`\n",
    "+ storage: set the type to SSD and the size of the storage to 20 GB\n",
    "\n",
    "with that done, let's go about installing and configuring `postgres`. \n",
    "\n",
    "## 6.2: installing `postgres`\n",
    "\n",
    "once that server starts up, `ssh` into it (note: for the `amazon linux ami` the username is `ec2-user`). let's start by installing postgres with the following:\n",
    "\n",
    "```bash\n",
    "sudo yum install postgresql postgresql-server postgresql-devel postgresql-contrib postgresql-docs\n",
    "sudo service postgresql initdb\n",
    "sudo vim /var/lib/pgsql9/data/pg_hba.conf\n",
    "```\n",
    "\n",
    "## 6.3: configuring `postgres`\n",
    "\n",
    "oh fun -- remember `vim`?!\n",
    "\n",
    "we need to tell the `postgres` service to allow you to log in as the `postgres` user (which is bad practice, but let's do it for fun anyway). toward the bottom of the file you just opened in `vim` there is a pair of lines that look like the following:\n",
    "\n",
    "```conf\n",
    "# \"local\" is for Unix domain socket connections only\n",
    "local   all             all                                     peer\n",
    "```\n",
    "\n",
    "these are saying that for people logged on to your machine (`local`), for all databases (the first `all`) and for all database usernames (the second `all`), use `peer` authentication (that is, get the user's name according to the operating system (so for us, `ec2-user`) and attempt to log them in with that name).\n",
    "\n",
    "we will loosen up those restrictions by changing the `peer` authentication method to `trust`, which allows anyone who has made it onto our `ec2` instance (`local`) to view any database as any user (`all` and `all`).\n",
    "\n",
    "```conf\n",
    "# \"local\" is for Unix domain socket connections only\n",
    "local   all             all                                     trust\n",
    "```\n",
    "\n",
    "## 6.4: verifying our install\n",
    "\n",
    "once this edit has been made, you can finally start the server and log in to your own `postgres` database server:\n",
    "\n",
    "```bash\n",
    "sudo service postgresql restart\n",
    "\n",
    "# only one user exists right now: postgres\n",
    "psql -U postgres\n",
    "```\n",
    "\n",
    "the output of that should just be a `psql` prompt:\n",
    "\n",
    "```\n",
    "psql (9.2.24)\n",
    "Type \"help\" for help.\n",
    "\n",
    "postgres=# \n",
    "```\n",
    "\n",
    "you're good! feel free to exit that `psql` shell with `\\q`\n",
    "\n",
    "\n",
    "## 6.5: installing `psycopg2`\n",
    "\n",
    "one last thing: let's install `psycopg2`. execute the following:\n",
    "\n",
    "```bash\n",
    "cd /tmp\n",
    "wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "# answer \"yes\" to all questions\n",
    "bash Miniconda3-latest-Linux-x86_64.sh\n",
    "\n",
    "cd\n",
    "source ~/.bashrc\n",
    "conda install psycopg2\n",
    "```\n",
    "\n",
    "##### there is nothing to submit for this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 7: comparing bulk `csv` insert methods\n",
    "\n",
    "there are a few different ways to perform inserts of many records, and they have pretty drastically different performance. the fastest way to insert large quantities of data into a database is *usually* by utilizing a proprietary bulk insert command. for example, in\n",
    "\n",
    "+ `ms sql`: [a `BULK INSERT` command](https://docs.microsoft.com/en-us/sql/t-sql/statements/bulk-insert-transact-sql)\n",
    "+ `mysql`: [the `LOAD DATA INFILE` command](https://dev.mysql.com/doc/refman/5.6/en/load-data.html)\n",
    "+ `oracle`: using a special oracle tool [`sql*loader`](https://docs.oracle.com/cd/B19306_01/server.102/b14215/ldr_concepts.htm) to write a \"control\" file (to outline the import steps) and import the datafile (this is what you pay hundreds of thousands of dollars for)\n",
    "\n",
    "`postgres` has implemented both a standard [`sql COPY` command](https://www.postgresql.org/docs/current/static/sql-copy.html) and a `psql` [`\\copy` meta command](https://www.postgresql.org/docs/9.2/static/app-psql.html#APP-PSQL-META-COMMANDS-COPY) for this task.\n",
    "\n",
    "the former (`sql COPY` command) can only be executed by users with appropriate permissions, and *must* be done with a local file. since we are using an unaccessbile `ec2` instance, we are out of luck on that count.\n",
    "\n",
    "the latter (`\\copy psql` command) can be executed by a non-admin user, and can be run remotely. the drawback: it is executed on the client side, so we have to send the entire inserted file over the wire.\n",
    "\n",
    "let's try out a few different options to see how considerable the speed differences are. first, though, we need to do some setup.\n",
    "\n",
    "\n",
    "## 7.1: prepping\n",
    "\n",
    "we will be performing this exercise from the `ec2` server we created above (and on which we installed `postgres`). `ssh` into that `ec2` server and take care of a few setup options.\n",
    "\n",
    "\n",
    "### 7.1.1: getting data\n",
    "\n",
    "download the bulk `csv` of `wmata` train position data which we intend to upload to our various `postgres` servers:\n",
    "\n",
    "```bash\n",
    "cd /tmp\n",
    "wget https://s3.amazonaws.com/shared.rzl.gu511.com/train_positions/train_positions.csv\n",
    "chmod a+r train_positions.csv\n",
    "cd ~\n",
    "```\n",
    "\n",
    "### 7.1.2: getting the `python` code\n",
    "\n",
    "I've written a short snippet of `python` code to\n",
    "\n",
    "0. mark the starting time\n",
    "1. create a connection to your `rds` database\n",
    "2. create a csv dictreader to read lines from `train_positions.csv`\n",
    "3. iterate through those csv lines and insert each of them one at a time\n",
    "    1. using a parameterized sql insert statement\n",
    "4. mark the ending time\n",
    "5. print the total time it took to insert the records to the screen\n",
    "\n",
    "download it to your `ec2` instance and review it:\n",
    "\n",
    "```bash\n",
    "cd ~\n",
    "wget https://s3.amazonaws.com/shared.rzl.gu511.com/train_positions/bulkinsert.py\n",
    "```\n",
    "\n",
    "\n",
    "### 7.1.3: creating an empty table on both dbs\n",
    "\n",
    "you can log into your `ec2` `postgres` server with\n",
    "\n",
    "```bash\n",
    "psql -U postgres\n",
    "```\n",
    "\n",
    "and you can log into your `rds` server with\n",
    "\n",
    "```bash\n",
    "psql --host YOUR_RDS_ENDPOINT --port YOUR_RDS_PORT --dbname YOUR_DB_NAME --user YOUR_MASTER_USER_NAME\n",
    "```\n",
    "\n",
    "log into each and execute the following `sql` code:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE train_positions (   \n",
    "    carcount real                \n",
    "    , circuitid real             \n",
    "    , destinationstationcode text\n",
    "    , directionnum real          \n",
    "    , linecode text              \n",
    "    , secondsatlocation real     \n",
    "    , servicetype text           \n",
    "    , trainid text               \n",
    "    , timestamp timestamp        \n",
    ");                               \n",
    "```\n",
    "\n",
    "you should get\n",
    "\n",
    "```bash\n",
    "CREATE TABLE\n",
    "```\n",
    "\n",
    "as a result of that command on either server. anything else may be an error.\n",
    "\n",
    "create this empty table on both servers!\n",
    "\n",
    "\n",
    "\n",
    "## 7.2: inserting in batches via `python` (remote)\n",
    "\n",
    "the first way we will attempt to insert these records is to use the `psycopg2` library to `INSERT` records in batches of 100 at a time. \n",
    "\n",
    "assuming your working directory is the one in which you downloaded the file `bulkinsert.py` above, run the following from your `ec2` server's terminal:\n",
    "\n",
    "```bash\n",
    "python bulkinsert.py --host YOUR_RDS_ENDPOINT --port YOUR_RDS_PORT --dbname YOUR_DB_NAME --user YOUR_MASTER_USER_NAME --fcsv /tmp/train_positions.csv\n",
    "```\n",
    "\n",
    "note how long this process took (it will take minutes, not seconds or hours).\n",
    "\n",
    "\n",
    "## 7.3: `psql` `\\copy` command (remote)\n",
    "\n",
    "what the previous command did was something pretty common: given a `csv` of records that matches the schema of the table we created, load all of the records. as it happens, this is so common that most relational databases have implemented shortcuts for doing exactly that.\n",
    "\n",
    "one such shortcut is the `psql` command `\\copy`. let's use the `\\copy` command to copy this *local* `csv` file to our *remote* `rds` database.\n",
    "\n",
    "on your `ec2` server, open a `psql` shell connection to your `rds` instance:\n",
    "\n",
    "```bash\n",
    "psql --host YOUR_RDS_ENDPOINT --port YOUR_RDS_PORT --dbname YOUR_DB_NAME --user YOUR_MASTER_USER_NAME\n",
    "```\n",
    "\n",
    "once in, execute the following `psql` command to turn on query timing (this will measure how long a query takes for us:\n",
    "\n",
    "```sql\n",
    "\\timing\n",
    "```\n",
    "\n",
    "now, execute the `\\copy` command:\n",
    "\n",
    "```sql\n",
    "\\copy train_positions from /tmp/train_positions.csv with delimiter as ',' null as '' csv header;\n",
    "```\n",
    "\n",
    "note how long this process took.\n",
    "\n",
    "after this has been run, clean up after yourself again:\n",
    "\n",
    "```sql\n",
    "DELETE FROM train_positions;\n",
    "```\n",
    "\n",
    "and then close the `psql` shell session:\n",
    "\n",
    "```sql\n",
    "\\q\n",
    "```\n",
    "\n",
    "\n",
    "## 7.4: `sql` `COPY` command (local)\n",
    "\n",
    "finally, let's mimic the process of doing a bulk load from a *local* file to a *local* `postgres` server. as I wrote above, this requires a superuser (e.g. user `postgres`) and that the file is on the same computer as the database server. this is exactly the scenario we set up on our `ec2` server.\n",
    "\n",
    "open a `psql` shell connected to your *local* (to `ec2`) `postgres` server:\n",
    "\n",
    "```bash\n",
    "psql -U postgres\n",
    "```\n",
    "\n",
    "again, turn on timing:\n",
    "\n",
    "```sql\n",
    "\\timing\n",
    "```\n",
    "\n",
    "and finally, copy the file into the table `train_positions`\n",
    "\n",
    "```sql\n",
    "COPY train_positions\n",
    "FROM '/tmp/train_positions.csv'\n",
    "WITH (\n",
    "    FORMAT csv\n",
    "    , DELIMITER ','\n",
    "    , NULL ''\n",
    "    , HEADER TRUE\n",
    ");\n",
    "```\n",
    "\n",
    "note how long this process took.\n",
    "\n",
    "after this has been run, clean up after yourself again:\n",
    "\n",
    "```sql\n",
    "DELETE FROM train_positions;\n",
    "```\n",
    "\n",
    "and then close the `psql` shell session:\n",
    "\n",
    "```sql\n",
    "\\q\n",
    "```\n",
    "\n",
    "\n",
    "## 7.5: delivering you results\n",
    "\n",
    "put the three different processing times you found on this exercise into a `csv` called `bulk_insert_times.csv` which has the following format:\n",
    "\n",
    "| method                    | time_in_ms |\n",
    "|---------------------------|------------|\n",
    "| `psycopg2` batch `INSERT` |            | \n",
    "| `\\copy`                   |            |\n",
    "| `COPY`                    |            |\n",
    "\n",
    "please report all the times you received in `ms` (this is the unit given for `\\timing`, as well as the `python` script I wrote).\n",
    "\n",
    "\n",
    "##### upload `bulk_insert_times.csv` to the `s3` homework bucket you created last week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 8: terminate your `postgres` `ec2` instance\n",
    "\n",
    "via the `aws` `ec2` web console, terminate the `ec2` instance we created above for your `postgres` database\n",
    "\n",
    "##### there is nothing to submit for this exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 9: wmata data: `create table`\n",
    "\n",
    "I've been downloading `wmata` metro train position information every 10 seconds since late 2016. at this point, I have... a *lot* of individual records of train locations. I have put 20,000,000 of those records into a `csv` and shared it via `s3`, and we're going to load them into a `redshift` instance and do some quick querying.\n",
    "\n",
    "in order to load this `csv` into `redshift` there must be a destination table to load it into. our first step must be to create that table.\n",
    "\n",
    "\n",
    "## 9.1: understanding the data\n",
    "\n",
    "the first ten records of this `csv` are:\n",
    "\n",
    "```csv\n",
    "carcount,circuitid,destinationstationcode,directionnum,linecode,secondsatlocation,servicetype,trainid,timestamp\n",
    "8,2009,F11,2,GR,42,Normal,067,2017-02-20 01:16:02.557021\n",
    "6,1912,F11,2,GR,0,Normal,175,2017-02-20 01:16:02.557021\n",
    "6,1480,D13,1,OR,0,Normal,182,2017-02-20 01:16:02.557021\n",
    "6,2786,D13,1,OR,0,Normal,234,2017-02-20 01:16:02.557021\n",
    "6,1522,D13,1,OR,48,Normal,272,2017-02-20 01:16:02.557021\n",
    "6,1384,D13,1,OR,5,Normal,342,2017-02-20 01:16:02.557021\n",
    "8,2890,D13,1,OR,0,Normal,390,2017-02-20 01:16:02.557021\n",
    "6,1710,K08,2,OR,615,Normal,061,2017-02-20 01:16:02.557021\n",
    "6,1288,K08,2,OR,5,Normal,160,2017-02-20 01:16:02.557021\n",
    "```\n",
    "\n",
    "you are welcome to download this information to investigate, but it is not necessary for this assignment. you can download it via\n",
    "\n",
    "```sh\n",
    "wget https://s3.amazonaws.com/shared.rzl.gu511.com/train_positions/train_positions.csv\n",
    "```\n",
    "\n",
    "typically our first questions when creating a new table would be\n",
    "\n",
    "1. what are the names of our columns?\n",
    "1. what are the data types of our columns?\n",
    "1. are any of them appropriate `distkey` or `sortkey` values? what is our distribution strategy?\n",
    "\n",
    "\n",
    "### 9.1.1: column names\n",
    "\n",
    "this one is simple: the last column name -- `timestamp` -- is a reserved keyword in `redshift` / `sql` (it is one of the data types). we could keep it as is, but then [we would need to escape it](https://docs.aws.amazon.com/redshift/latest/dg/r_pg_keywords.html) in queries (`select \"timestamp\", ...`). I'm lazy and don't want to do that.\n",
    "\n",
    "so when we create that column, let's rename it to `asoftime`.\n",
    "\n",
    "\n",
    "### 9.1.2: data types\n",
    "\n",
    "the data types break down roughly as:\n",
    "\n",
    "+ `integer`: `carcount`, `circuitid`, `directionnum`, `secondsatlocation`, `trainid`\n",
    "+ `varchar(4)`: `desintationstationcode`, `linecode`\n",
    "+ `varchar(100)`: `servicetype`\n",
    "+ `asoftime`: `timestamp`\n",
    "\n",
    "*note: the numeric lengths of the `varchar` selections above are not particularly empirical; other values may have been more appropriate.*\n",
    "\n",
    "\n",
    "### 9.1.3: key choice and distribution strategy\n",
    "\n",
    "I'll cut to the chase: this batch of records doesn't have a good `distkey` candidate. \n",
    "\n",
    "a good `distkey` candidate would have *high cardinality* (many values) and relatively even distribution of the number of records per `distkey` value. having an imballance -- e.g. 75% of records having a value of 0 or `NULL` -- is called **skew** and will lead to certain slices of data and certain workers being much more taxed than others (worse than even distribution!).\n",
    "\n",
    "for `sortkey`, however, we do have a natural sort value -- our `asoftime` column.\n",
    "\n",
    "when you `create` this table, do not set a `distkey` but do set `asoftime` as the `sortkey`.\n",
    "\n",
    "\n",
    "## 9.2: the `create table` statement\n",
    "\n",
    "write a [`create table`](https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_TABLE_NEW.html) statement that will create a new table that\n",
    "\n",
    "1. is named `trainpositions`\n",
    "1. keeps the same column names as the `csv` except for `timestamp` which is renamed `asoftime`\n",
    "1. uses the provided data types\n",
    "1. has no `distkey` and has `asoftime` as a `sortkey`\n",
    "\n",
    "save that statement in a file called `create_trainpositions.sql`\n",
    "\n",
    "\n",
    "##### upload `create_trainpositions.sql` to the `s3` homework bucket you created last week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 10: wmata data: `copy`\n",
    "\n",
    "now that we have created our `trainpositions` table, we must copy our `csv` data into it. at this stage, the questions shift over to the details about the format of the `csv` we wish to import and the permissions we will need to import it, and subsequently how we can configure those values or behaviors in our `copy` command (this is no different than reading a `csv` into `pandas` or an `R` dataframe).\n",
    "\n",
    "for now, there are a few things we must tackle:\n",
    "\n",
    "1. our `csv` file has a comma as a delimiter\n",
    "1. our `csv` file has a header row\n",
    "1. our `csv` is publically available on `s3` at `s3://shared.rzl.gu511.com/train_positions/train_positions.csv`\n",
    "    1. this means that in order to execute our `copy` command, `redshift` will need to know about credentials allowing it to access `s3`\n",
    "1. our `csv` is in the `us-east-1` region\n",
    "\n",
    "use [the `redshift` `copy` command documentation](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html) to figure out how to specify the above configuration options.\n",
    "\n",
    "create a `copy` statement that will copy the `csv` into `trainpositions` and save it to a file named `copy_trainpositions.sql`\n",
    "\n",
    "\n",
    "##### upload `copy_trainpositions.sql` to the `s3` homework bucket you created last week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 11: wmata data: querying\n",
    "\n",
    "*note: if you are stuck on the previous two questions and unable to create your own `trainpositions` table, I have a `redshift` cluster you may connect to and query to complete this portion. the connection information will be sent to you as part of the homework announcement email*\n",
    "\n",
    "at this point you should be free to query these records. fill in the following skeleton of `sql` queries by replacing any block like\n",
    "\n",
    "```sql\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "```\n",
    "\n",
    "with a single `sql` query. name the completed file `wmata_queries.sql`\n",
    "\n",
    "as a guide, I have provided the output of the first three expected rows of the properly structured query in the comments after each `FILL THIS IN` block.\n",
    "\n",
    "```sql\n",
    "/* -------------------------------------------------------------------------- */\n",
    "/*  what are the time bounds of our dataset (the first and last timestamp\n",
    "    values?\n",
    "*/\n",
    "\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "\n",
    "/*  only one record, not three:\n",
    "\n",
    "    max: 2017-03-20 19:43:18.308864\n",
    "    min: 2017-02-20 01:16:02.557021\n",
    "*/\n",
    "\n",
    "\n",
    "/* -------------------------------------------------------------------------- */\n",
    "/*  among trains with a \"Normal\" servicetype, what was the longest single wait\n",
    "    (`secondsatlocation`) among all trains?\n",
    "    \n",
    "    does that seem reasonable?\n",
    "*/\n",
    "\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "\n",
    "/*  all I can say here is that 17 hours is 61,200 seconds, and wmata's \n",
    "    documentation says that `secondsatlocation` is:\n",
    "    \n",
    "    > Approximate \"dwell time\". This is not an exact value, but can be used to \n",
    "    > determine how long a train has been reported at the same track circuit.\n",
    "    \n",
    "    so reasonable is in the eye of the beholder here.\n",
    "*/\n",
    "\n",
    "\n",
    "/* -------------------------------------------------------------------------- */\n",
    "/*  within a `select` statement, we can use\n",
    "\n",
    "    trunc(asoftime) as asofdate\n",
    "    \n",
    "    to *truncate* the asoftime value to a date. for example:\n",
    "*/\n",
    "\n",
    "SELECT TRUNC(asoftime) AS asofdate\n",
    "FROM trainpositions\n",
    "LIMIT 5;\n",
    "\n",
    "/*  count how many position records there are for each day, and order results\n",
    "    from most to least records\n",
    "*/\n",
    "\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "\n",
    "/*  2017-03-09\t819830\n",
    "    2017-03-08\t816153\n",
    "    2017-03-10\t793110\n",
    "*/\n",
    "\n",
    "\n",
    "/* -------------------------------------------------------------------------- */\n",
    "/*  wmata defines a `trainid` as a\n",
    "\n",
    "    > Uniquely identifiable internal train identifier.\n",
    "    \n",
    "    so at any given moment `trainid` should uniquely define one of the trains on\n",
    "    the track. frustratingly, these ids are not consistent from day to day, and\n",
    "    can be re-used intraday.\n",
    "    \n",
    "    ignore that complication for now!\n",
    "    \n",
    "    count how many distinct trainids are recorded on each day, and order results\n",
    "    from most to least\n",
    "*/\n",
    "\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "\n",
    "/*  2017-02-28\t510\n",
    "    2017-03-03\t509\n",
    "    2017-03-02\t508\n",
    "*/\n",
    "\n",
    "\n",
    "/* -------------------------------------------------------------------------- */\n",
    "/*  within a `select` statement, we can use\n",
    "\n",
    "    date_part(dow, asoftime) as dow\n",
    "    \n",
    "    to get the numeric day of the week (0-6, staring with Sunday) of the\n",
    "    provided timestamp value. for example\n",
    "*/\n",
    "\n",
    "SELECT DATE_PART(dow,asoftime) AS dow\n",
    "FROM trainpositions\n",
    "LIMIT 5;\n",
    "\n",
    "/*  count how many records were reported on each day of the week, and order\n",
    "    results from most to least total records records\n",
    "*/\n",
    "\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "\n",
    "/*  1.0\t3400605\n",
    "    5.0\t3093400\n",
    "    4.0\t3083713\n",
    "*/\n",
    "\n",
    "\n",
    "/* -------------------------------------------------------------------------- */\n",
    "/*  what is the busiest line?\n",
    "\n",
    "    count how many records were reported on each non-empty linecode, and order\n",
    "    results from most to least total records records\n",
    "*/\n",
    "\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "\n",
    "/*  RD\t4372172\n",
    "    SV\t2624013\n",
    "    OR\t2338064\n",
    "*/\n",
    "\n",
    "\n",
    "/* -------------------------------------------------------------------------- */\n",
    "/*  where are trains going?\n",
    "\n",
    "    count how many records were reported heading to each non-empty \n",
    "    destinationstationcode, and order results from most to least total records.\n",
    "    \n",
    "    note: you can look up the destination station codes directly here:\n",
    "    https://developer.wmata.com/docs/services/5476364f031f590f38092507/operations/5476364f031f5909e4fe3311\n",
    "*/\n",
    "\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "\n",
    "/*  A15\t1790554\n",
    "    G05\t1724472\n",
    "    B11\t1477805\n",
    "*/\n",
    "\n",
    "\n",
    "/* -------------------------------------------------------------------------- */\n",
    "/*  what's the most frustrating train line?\n",
    "\n",
    "    let's say that what most frustrates you is holding in place on your ride.\n",
    "    which line should you avoid?\n",
    "    \n",
    "    let's split the world into holds over 30 minutes (30 * 60 = 1,800 seconds)\n",
    "    and those under\n",
    "    \n",
    "    find the average time holding (secondsatlocation) for each non-null \n",
    "    linecode for each under-30-minute hold, and order results from longest to \n",
    "    shortest average hold time\n",
    "    \n",
    "    note: these are sequential values measured every 10 seconds, so a hold of\n",
    "    over 10 seconds will result in multiple records (i.e. we are double \n",
    "    counting). ideally we would find the last record in such a hold sequence,\n",
    "    but that is not easy to do, so we will ignore this complication for the\n",
    "    time being and hope that the problem affects all lines equally.\n",
    "*/\n",
    "\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "\n",
    "/*  BL\t88\n",
    "    YL\t60\n",
    "    OR\t52\n",
    "    GR\t48\n",
    "    SV\t41\n",
    "    RD\t33\n",
    "*/\n",
    "\n",
    "/* -------------------------------------------------------------------------- */\n",
    "/*  let's look at the hold incidents lasting over 1,800 seconds. how many did\n",
    "    each line have?\n",
    "    \n",
    "    here overcounting would certainly be a problem, so let's avoid that. do the\n",
    "    following:\n",
    "    \n",
    "    1. in a sub-query (that is, a\n",
    "    \n",
    "        FROM (\n",
    "            SELECT ...\n",
    "            FROM ...\n",
    "            WHERE ...\n",
    "        ) A\n",
    "        \n",
    "       statement), filter down trainpositions to only records with non-empty\n",
    "       linecodes and  secondsatlocation over 1800 seconds, and select only \n",
    "       distinct pairs of trainid, date (not timestamp) and linecode\n",
    "    2. aggreagte that subquery to count the number of occurrences per linecode\n",
    "    \n",
    "    we will do this in two steps\n",
    "*/\n",
    "\n",
    "/*  make the subquery: a list of distinct trainid, date, linecode trios for all\n",
    "    holds over 1800 seconds and non-empty linecodes\n",
    "*/\n",
    "\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "\n",
    "/* this should return 575 lines, and the first three are\n",
    "\n",
    "    492\t2017-02-20\tSV\n",
    "    91\t2017-02-20\tRD\n",
    "    228\t2017-02-20\tGR\n",
    "*/\n",
    "\n",
    "/*  use the query above to count the number of records per linecode  */\n",
    "\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "FROM (\n",
    "    -- USE THE SUBQUERY ABOVE!!!\n",
    "    -- USE THE SUBQUERY ABOVE!!!\n",
    "    -- USE THE SUBQUERY ABOVE!!!\n",
    ")\n",
    "-- FILL THIS IN !!!\n",
    "-- FILL THIS IN !!!\n",
    "\n",
    "/*  OR\t108\n",
    "    SV\t107\n",
    "    RD\t80\n",
    "    GR\t51\n",
    "    YL\t127\n",
    "    BL\t102\n",
    "*/\n",
    "```\n",
    "\n",
    "##### upload `wmata_queries.sql` to the `s3` homework bucket you created last week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 12: conneting to `redshift` from `python`\n",
    "\n",
    "under the hood, `redshift` is just a glorified fork of `postgres` -- we can connect to it in exactly the same way we connected to our `postgres` `rds` instances previously.\n",
    "\n",
    "\n",
    "## 12.1: collect connection information\n",
    "\n",
    "from your `redshift` cluster's description page, grab the following information:\n",
    "\n",
    "+ host name\n",
    "+ port\n",
    "+ user name\n",
    "+ database name\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=14U7pR2jeN7vR9g1PsIdUjpPrFDeG8bQs\" width=\"600px\"></div><br>\n",
    "\n",
    "you will also need the password, but that you have to remember!\n",
    "\n",
    "\n",
    "## 12.2: make a connection\n",
    "\n",
    "using the information above, fill in the following `python` code and save the results as `redshift.py`\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "\n",
    "def query_redshift(q, hostname, port, username, password, dbname):\n",
    "    # use sqlalchemy to create an engine\n",
    "    # ---------------- #\n",
    "    # FILL THIS IN !!! #\n",
    "    # ---------------- #\n",
    "    \n",
    "    # execute the query using pandas\n",
    "    # ---------------- #\n",
    "    # FILL THIS IN !!! #\n",
    "    # ---------------- #\n",
    "    \n",
    "    # return the dataframe we just created\n",
    "    return df\n",
    "\n",
    "\n",
    "def test(hostname, port, username, password, dbname):\n",
    "    q = \"\"\"SELECT * \n",
    "    FROM pg_table_def \n",
    "    WHERE tablename = 'trainpositions'\"\"\" \n",
    "    df = query_redshift(q, hostname, port, username, password, dbname)\n",
    "    \n",
    "    assert df.shape == (9, 8)\n",
    "    try:\n",
    "        assert df.iloc[6].type == 'character varying(100)'\n",
    "    except AssertionError:\n",
    "        print('what sql datatype did you use for the servicetype field?')\n",
    "        raise\n",
    "```\n",
    "\n",
    "##### upload `redshift.py` to the `s3` homework bucket you created last week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 13: resolving an issue with a `commit` to `master`\n",
    "\n",
    "`github` -- *not* `git` itself -- has a concept of \"issues\". issues are a way to record \"issues\" you have with the code as it is, including feature requests, bugs, and improvements. you can view these from the \"issues\" tab on the main repository page:\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=1SYqdKMaNsFvWxEsyAFARir4lZkLZIQ_U\"></div>\n",
    "\n",
    "these provide a great way for people to communicate and discuss their development efforts. you should use them!\n",
    "\n",
    "`github` also has other integrations with `issues`, including (importantly) the ability to *close* issues by referencing them in commit messages. that's what we're going to do.\n",
    "\n",
    "I have already added an issue to your repositories requesting a simple change be made. the issue's title is **pin version numbers in requirements file**, and the goal is to hard-code the version numbers of the packages we want users to install and use to run the `dspipeline.py` file.\n",
    "\n",
    "\n",
    "## 13.1: viewing and assigning the issue\n",
    "\n",
    "log in to `github` and click on the \"issues\" tab, and open the issue I created for you. in particular, I want you to **assign** it to yourself -- click on the \"assign yourself\" link on the issues page\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=1AHYHy4b_A5k4CFC6IUj6Fyyx5_ICnNOz\"></div>\n",
    "\n",
    "\n",
    "\n",
    "## 13.2: editing the `requirements.txt` file\n",
    "\n",
    "locally, edit the `requirements.txt` file to read\n",
    "\n",
    "```\n",
    "numpy==1.15.2\n",
    "pandas==0.23.4\n",
    "plotly==3.3.0\n",
    "scikit-learn==0.20.0\n",
    "```\n",
    "\n",
    "hold off on committing for just a moment\n",
    "\n",
    "\n",
    "## 13.3: background on resolving `github` issues with `commit` messages\n",
    "\n",
    "`github` [allows users to resolve and close issues with `commit` message](https://help.github.com/articles/closing-issues-using-keywords/). read the documentation on that page!\n",
    "\n",
    "if you make a `commit` message to any branch (including `master`) that uses keywords like \"fixes\" or \"resolves\" and references the issue by number, `github` will link that commit and the named issue. if that commit is to the `master` branch, `github` will automatically close the issue with a reference to the `commit` `sha`:\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=1vn8HX4wkn2HhjOBGu842wMuVQSWxHtIu\" width=\"700px\"></div>\n",
    "\n",
    "if the `commit` is made to a non-`master` branch, it will allow users to create something called a \"pull request\" -- more on that in a future exercise!\n",
    "\n",
    "\n",
    "## 13.4: actually resolving `github` issues with `commit` messages\n",
    "\n",
    "`add` and `commit` your update to `requirements.txt` to the `master` branch with commit message\n",
    "\n",
    "```\n",
    "requirements.txt: pin versions, fixes #YOUR_ISSUE_NUMBER\n",
    "```\n",
    "\n",
    "where you replace `YOUR_ISSUE_NUMBER` with the number of your issue as seen in `github`:\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=1otKcV-eudPkQm-EbJ3alevItgcHhIl_O\" width=\"500px\"></div>\n",
    "\n",
    "your issue number is *probably* 1, but double-check! for example, my commit message was\n",
    "\n",
    "```\n",
    "requirements.txt: pin versions, fixes #1\n",
    "```\n",
    "\n",
    "after you've `commit`ed, `push` to `origin` `master`\n",
    "\n",
    "\n",
    "## 13.5: verify that the issue in your `github` repo is closed\n",
    "\n",
    "check your issue page in `github` and verify that it appears closed and references the comit message you made:\n",
    "\n",
    "<br><div align=\"center\"><img src=\"http://drive.google.com/uc?export=view&id=1vn8HX4wkn2HhjOBGu842wMuVQSWxHtIu\" width=\"700px\"></div>\n",
    "\n",
    "\n",
    "##### submission will be verified via `github`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
